{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dipta007/hate-speech-election-2020/blob/main/Hate_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-Uy6wwZckGg",
    "outputId": "19077aa5-01d9-4b51-9f89-6ff5a32f6b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (4.2.2)\n",
      "Requirement already satisfied: requests in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: filelock in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: numpy in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: packaging in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: six in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 7.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from datasets) (1.2.3)\n",
      "Collecting pyarrow>=1.0.0<4.0.0\n",
      "  Downloading pyarrow-4.0.0-cp37-cp37m-manylinux2014_x86_64.whl (21.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.8 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py37-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from datasets) (3.7.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from datasets) (1.20.2)\n",
      "Requirement already satisfied: packaging in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from datasets) (20.9)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.4.0-py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: filelock in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: dill, pyarrow, tqdm, multiprocess, huggingface-hub, xxhash, fsspec, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n",
      "      Successfully uninstalled tqdm-4.59.0\n",
      "Successfully installed datasets-1.6.2 dill-0.3.3 fsspec-2021.4.0 huggingface-hub-0.0.8 multiprocess-0.70.11.1 pyarrow-4.0.0 tqdm-4.49.0 xxhash-2.0.2\n",
      "Requirement already satisfied: pyspellchecker in /home/dipta007/miniconda3/envs/ml/lib/python3.7/site-packages (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a40uLozp6ah7",
    "outputId": "a562a600-6e21-4c59-bd86-8a62874e2045"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dipta007/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/dipta007/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dipta007/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Small Libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab, GloVe\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, set_seed\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6kwUXpaSQIKa"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z5HK17tCbCOs"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 4\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IpwGOLY-b2YP"
   },
   "outputs": [],
   "source": [
    "def get_model_tokenizer(name):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            name,\n",
    "            # num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "          )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yl1KDq214TAW"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer(\"mrm8488/distilroberta-finetuned-tweets-hate-speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p2V8b8xKfWWU"
   },
   "outputs": [],
   "source": [
    "# for param in model.roberta.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in model.classifier.parameters():\n",
    "#     print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_nATxfc3Uqp",
    "outputId": "c19a0f40-0157-45a9-b091-faf252133c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hatexplain (/root/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/802fcd855438812094e336cea509c99b04b890e4e0846c0385877ee2c7361e93)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hatexplain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3pdoCWCR3fsu",
    "outputId": "6366c71e-6cb2-4246-a3b1-1633b62d73f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'u really think i would not have been raped by feral hindu or muslim back in india or bangladesh and a neo nazi would rape me as well just to see me cry'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(dataset[\"train\"][0][\"post_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfb7_Sbf4kGs",
    "outputId": "a2f27403-4a5e-48b6-96c8-1dca299a585e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"annotators\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEauMs_2epHq",
    "outputId": "df8744c5-0a79-4345-c3b7-aa9e1dbafb1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "a nigger bich will never have strait hair so keep wearing dat horsehair weave\n",
      "\n",
      "[0, 0, 0]\n",
      "apparently some vile nigger animals kidnapped a white special needs teen and tortured and scalped him while shouting fuck white people fuck trump are you being armed and are you shooting in the head when these zombies approach\n",
      "\n",
      "[0, 0, 0]\n",
      "<user> you need potassium eat a banana you fucking nigger\n",
      "\n",
      "[2, 0, 0]\n",
      "well then send some coon estrogen to my wife she turning more liberal breath by breath due to fake news i do not know how much more i can take hell between her and my two daughters hormones i have started growing breast tissue like pseudo naturally becoming bruce jenner\n",
      "\n",
      "[0, 0, 0]\n",
      "it high time we hindus should think that these filthy muslims should be allowed to live here or not kamleshtiwari ypg yourprophetisgay\n",
      "\n",
      "[1, 1, 1]\n",
      "<user> why should you ban immigrants from operating small business every business start somewhere and tomorrow that will give people job\n",
      "\n",
      "[2, 0, 2]\n",
      "i am sure you are totally the alpha faggot at the standard fuckparties\n",
      "\n",
      "[2, 1, 2]\n",
      "<user> aint he bisexual an lgbt king of gays even his mind\n",
      "\n",
      "[2, 0, 2]\n",
      "you insecure lowlife broke ass desperate woman beating momma boy ass niggas are fucking sick get a grip on reality hoe dumbass dykes\n",
      "\n",
      "[0, 2, 0]\n",
      "i guess this nigger and his mudshark are unaware of the merchant this is some hilarious trolling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(40, 50):\n",
    "  print(dataset[\"train\"][i][\"annotators\"][\"label\"])\n",
    "  print(\" \".join(dataset[\"train\"][i][\"post_tokens\"]))\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ow250cpu6lZs"
   },
   "outputs": [],
   "source": [
    "def join_all(arr):\n",
    "  ret = []\n",
    "  for v in arr:\n",
    "    ret.append(\" \".join(v))\n",
    "  return ret\n",
    "\n",
    "train_encodings = tokenizer(join_all(dataset[\"train\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(join_all(dataset[\"validation\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(join_all(dataset[\"test\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyKvFyhl8A3q",
    "outputId": "88bab619-3113-41c4-a35e-0c7e291821c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15383"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4EJ7cThpFBEr"
   },
   "outputs": [],
   "source": [
    "class HateModel(torch.nn.Module):\n",
    "  def __init__(self, bert):\n",
    "    super(HateModel, self).__init__()\n",
    "    self.bert = bert\n",
    "    self.linear1 = torch.nn.Linear(2, 128)\n",
    "    self.linear2 = torch.nn.Linear(128, 3)\n",
    "\n",
    "  def forward(self, x, attention_mask, labels):\n",
    "    x = self.bert(x, attention_mask=attention_mask, labels=labels)\n",
    "    print(x)\n",
    "    x = F.relu(self.linear1(x[1]))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OJVkda7qn9pR"
   },
   "outputs": [],
   "source": [
    "class HateExplain(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, dataset):\n",
    "    self.dataset = dataset\n",
    "    self.encodings = encodings\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = self.get_label(self.dataset[idx][\"annotators\"][\"label\"])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def get_label(self, arr):\n",
    "    cnt = [0, 0, 0]\n",
    "    for v in arr:\n",
    "      cnt[v] += 1\n",
    "    \n",
    "    # 0 - hate\n",
    "    # 1 - normal\n",
    "    # 2 - offensive\n",
    "    return 1 if torch.argmax(torch.as_tensor(cnt)) != 1 else 0\n",
    "\n",
    "train_dataset = HateExplain(train_encodings, dataset[\"train\"])\n",
    "val_dataset = HateExplain(val_encodings, dataset[\"validation\"])\n",
    "test_dataset = HateExplain(test_encodings, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NKFy2s2MWOmb"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ls4Ff2lzPVeF"
   },
   "outputs": [],
   "source": [
    "def evaluate(type_of_data, dataloader, model, criterion, softmax=False):\n",
    "  predictions, actuals = torch.as_tensor([]).to(device), torch.as_tensor([]).to(device)\n",
    "  total_loss = 0\n",
    "  steps = 0\n",
    "  for batch in dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    y = labels\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    yhat = outputs[1]\n",
    "    loss = outputs[0]\n",
    "\n",
    "    # yhat = model(input_ids, attention_mask, labels)\n",
    "    # loss = criterion(yhat, y)\n",
    "\n",
    "    steps += 1\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if not softmax:\n",
    "      yhat = F.softmax(yhat, dim=1)\n",
    "      yhat = torch.argmax(yhat, dim=1)\n",
    "    # store\n",
    "    predictions = torch.cat([predictions, yhat])\n",
    "    actuals = torch.cat([actuals, y])\n",
    "      \n",
    "  actuals = actuals.cpu()\n",
    "  predictions = predictions.cpu()\n",
    "\n",
    "  acc = accuracy_score(actuals, predictions)\n",
    "  f1_micro = f1_score(actuals, predictions, average=\"micro\")\n",
    "  f1_macro = f1_score(actuals, predictions, average=\"macro\")\n",
    "  avg_loss = total_loss / steps\n",
    "\n",
    "  print(type_of_data, 'Acc', round(acc, 3), \"f1 micro\", round(f1_micro, 3), \"f1 macro\", round(f1_macro, 3), \"loss\", round(avg_loss, 3), end=\" || \")\n",
    "\n",
    "  return round(acc, 3), round(avg_loss, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DEs4IBXvnh6",
    "outputId": "4c2728c5-4d81-4829-8dec-a920138204d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training Acc 0.522 f1 micro 0.522 f1 macro 0.521 loss 1.908 || Validation Acc 0.53 f1 micro 0.53 f1 macro 0.529 loss 1.856 || \n",
      "Step 2: "
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "# hate_model = HateModel(model)\n",
    "# hate_model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "EPOCH = 400\n",
    "\n",
    "# Early Stopping\n",
    "es_score = float('inf')\n",
    "es_counter = 0\n",
    "es_patience = 2\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "  model.train(True)\n",
    "  for batch in train_loader:\n",
    "    break\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    y_pred = outputs[1]\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    print('Step', epoch + 1, end=\": \")\n",
    "    evaluate(\"Training\", train_loader, model, criterion, False)\n",
    "    val_acc, val_loss = evaluate(\"Validation\", val_loader, model, criterion, False)\n",
    "    print()\n",
    "\n",
    "    if val_loss < es_score:\n",
    "      es_score = val_loss\n",
    "      es_counter = 0\n",
    "      best_model = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "      es_counter += 1\n",
    "      if es_counter > es_patience:\n",
    "        print(f'Loop terminated at {epoch+1} with val_loss {val_loss} and val_acc {val_acc}')\n",
    "        model.load_state_dict(best_model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCsL1sDQ44Hr"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  evaluate(\"Testing\", test_loader, model, criterion, False)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyTM03nJ-ETN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPv+hnmEmorFI2Kqc1AFQ1w",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Hate Speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
