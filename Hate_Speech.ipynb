{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hate Speech.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPv+hnmEmorFI2Kqc1AFQ1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipta007/hate-speech-election-2020/blob/main/Hate_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Uy6wwZckGg",
        "outputId": "19077aa5-01d9-4b51-9f89-6ff5a32f6b20"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pyspellchecker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a40uLozp6ah7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a562a600-6e21-4c59-bd86-8a62874e2045"
      },
      "source": [
        "# Small Libs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "import random\n",
        "import string\n",
        "from spellchecker import SpellChecker\n",
        "import copy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Scikit Learn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab, GloVe\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize as nltk_tokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# HuggingFace\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, set_seed\n",
        "from datasets import load_dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kwUXpaSQIKa"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5HK17tCbCOs"
      },
      "source": [
        "RANDOM_SEED = 4\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "set_seed(RANDOM_SEED)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpwGOLY-b2YP"
      },
      "source": [
        "def get_model_tokenizer(name):\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            name,\n",
        "            # num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
        "            output_attentions = False, # Whether the model returns attentions weights.\n",
        "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "          )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "  return model, tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl1KDq214TAW"
      },
      "source": [
        "model, tokenizer = get_model_tokenizer(\"mrm8488/distilroberta-finetuned-tweets-hate-speech\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2V8b8xKfWWU"
      },
      "source": [
        "# for param in model.roberta.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# for param in model.classifier.parameters():\n",
        "#     print(param.requires_grad)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_nATxfc3Uqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19a0f40-0157-45a9-b091-faf252133c4e"
      },
      "source": [
        "dataset = load_dataset(\"hatexplain\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset hatexplain (/root/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/802fcd855438812094e336cea509c99b04b890e4e0846c0385877ee2c7361e93)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pdoCWCR3fsu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6366c71e-6cb2-4246-a3b1-1633b62d73f3"
      },
      "source": [
        "\" \".join(dataset[\"train\"][0][\"post_tokens\"])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'u really think i would not have been raped by feral hindu or muslim back in india or bangladesh and a neo nazi would rape me as well just to see me cry'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfb7_Sbf4kGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f27403-4a5e-48b6-96c8-1dca299a585e"
      },
      "source": [
        "dataset[\"train\"][0][\"annotators\"][\"label\"]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEauMs_2epHq",
        "outputId": "df8744c5-0a79-4345-c3b7-aa9e1dbafb1b"
      },
      "source": [
        "for i in range(40, 50):\n",
        "  print(dataset[\"train\"][i][\"annotators\"][\"label\"])\n",
        "  print(\" \".join(dataset[\"train\"][i][\"post_tokens\"]))\n",
        "  print(\"\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0]\n",
            "a nigger bich will never have strait hair so keep wearing dat horsehair weave\n",
            "\n",
            "[0, 0, 0]\n",
            "apparently some vile nigger animals kidnapped a white special needs teen and tortured and scalped him while shouting fuck white people fuck trump are you being armed and are you shooting in the head when these zombies approach\n",
            "\n",
            "[0, 0, 0]\n",
            "<user> you need potassium eat a banana you fucking nigger\n",
            "\n",
            "[2, 0, 0]\n",
            "well then send some coon estrogen to my wife she turning more liberal breath by breath due to fake news i do not know how much more i can take hell between her and my two daughters hormones i have started growing breast tissue like pseudo naturally becoming bruce jenner\n",
            "\n",
            "[0, 0, 0]\n",
            "it high time we hindus should think that these filthy muslims should be allowed to live here or not kamleshtiwari ypg yourprophetisgay\n",
            "\n",
            "[1, 1, 1]\n",
            "<user> why should you ban immigrants from operating small business every business start somewhere and tomorrow that will give people job\n",
            "\n",
            "[2, 0, 2]\n",
            "i am sure you are totally the alpha faggot at the standard fuckparties\n",
            "\n",
            "[2, 1, 2]\n",
            "<user> aint he bisexual an lgbt king of gays even his mind\n",
            "\n",
            "[2, 0, 2]\n",
            "you insecure lowlife broke ass desperate woman beating momma boy ass niggas are fucking sick get a grip on reality hoe dumbass dykes\n",
            "\n",
            "[0, 2, 0]\n",
            "i guess this nigger and his mudshark are unaware of the merchant this is some hilarious trolling\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow250cpu6lZs"
      },
      "source": [
        "def join_all(arr):\n",
        "  ret = []\n",
        "  for v in arr:\n",
        "    ret.append(\" \".join(v))\n",
        "  return ret\n",
        "\n",
        "train_encodings = tokenizer(join_all(dataset[\"train\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "val_encodings = tokenizer(join_all(dataset[\"validation\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(join_all(dataset[\"test\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyKvFyhl8A3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88bab619-3113-41c4-a35e-0c7e291821c8"
      },
      "source": [
        "len(dataset[\"train\"])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15383"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EJ7cThpFBEr"
      },
      "source": [
        "class HateModel(torch.nn.Module):\n",
        "  def __init__(self, bert):\n",
        "    super(HateModel, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.linear1 = torch.nn.Linear(2, 128)\n",
        "    self.linear2 = torch.nn.Linear(128, 3)\n",
        "\n",
        "  def forward(self, x, attention_mask, labels):\n",
        "    x = self.bert(x, attention_mask=attention_mask, labels=labels)\n",
        "    print(x)\n",
        "    x = F.relu(self.linear1(x[1]))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJVkda7qn9pR"
      },
      "source": [
        "class HateExplain(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, dataset):\n",
        "    self.dataset = dataset\n",
        "    self.encodings = encodings\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = self.get_label(self.dataset[idx][\"annotators\"][\"label\"])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def get_label(self, arr):\n",
        "    cnt = [0, 0, 0]\n",
        "    for v in arr:\n",
        "      cnt[v] += 1\n",
        "    \n",
        "    # 0 - hate\n",
        "    # 1 - normal\n",
        "    # 2 - offensive\n",
        "    return 1 if torch.argmax(torch.as_tensor(cnt)) != 1 else 0\n",
        "\n",
        "train_dataset = HateExplain(train_encodings, dataset[\"train\"])\n",
        "val_dataset = HateExplain(val_encodings, dataset[\"validation\"])\n",
        "test_dataset = HateExplain(test_encodings, dataset[\"test\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKFy2s2MWOmb"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls4Ff2lzPVeF"
      },
      "source": [
        "def evaluate(type_of_data, dataloader, model, criterion, softmax=False):\n",
        "  predictions, actuals = torch.as_tensor([]).to(device), torch.as_tensor([]).to(device)\n",
        "  total_loss = 0\n",
        "  steps = 0\n",
        "  for batch in dataloader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    y = labels\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    yhat = outputs[1]\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # yhat = model(input_ids, attention_mask, labels)\n",
        "    # loss = criterion(yhat, y)\n",
        "\n",
        "    steps += 1\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if not softmax:\n",
        "      yhat = F.softmax(yhat, dim=1)\n",
        "      yhat = torch.argmax(yhat, dim=1)\n",
        "    # store\n",
        "    predictions = torch.cat([predictions, yhat])\n",
        "    actuals = torch.cat([actuals, y])\n",
        "      \n",
        "  actuals = actuals.cpu()\n",
        "  predictions = predictions.cpu()\n",
        "\n",
        "  acc = accuracy_score(actuals, predictions)\n",
        "  f1_micro = f1_score(actuals, predictions, average=\"micro\")\n",
        "  f1_macro = f1_score(actuals, predictions, average=\"macro\")\n",
        "  avg_loss = total_loss / steps\n",
        "\n",
        "  print(type_of_data, 'Acc', round(acc, 3), \"f1 micro\", round(f1_micro, 3), \"f1 macro\", round(f1_macro, 3), \"loss\", round(avg_loss, 3), end=\" || \")\n",
        "\n",
        "  return round(acc, 3), round(avg_loss, 3)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DEs4IBXvnh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2728c5-4d81-4829-8dec-a920138204d9"
      },
      "source": [
        "print(device)\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "# hate_model = HateModel(model)\n",
        "# hate_model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "EPOCH = 400\n",
        "\n",
        "# Early Stopping\n",
        "es_score = float('inf')\n",
        "es_counter = 0\n",
        "es_patience = 2\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  model.train(True)\n",
        "  for batch in train_loader:\n",
        "    break\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    \n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    y_pred = outputs[1]\n",
        "    loss = outputs[0]\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    print('Step', epoch + 1, end=\": \")\n",
        "    evaluate(\"Training\", train_loader, model, criterion, False)\n",
        "    val_acc, val_loss = evaluate(\"Validation\", val_loader, model, criterion, False)\n",
        "    print()\n",
        "\n",
        "    if val_loss < es_score:\n",
        "      es_score = val_loss\n",
        "      es_counter = 0\n",
        "      best_model = copy.deepcopy(model.state_dict())\n",
        "    else:\n",
        "      es_counter += 1\n",
        "      if es_counter > es_patience:\n",
        "        print(f'Loop terminated at {epoch+1} with val_loss {val_loss} and val_acc {val_acc}')\n",
        "        model.load_state_dict(best_model)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Training Acc 0.522 f1 micro 0.522 f1 macro 0.521 loss 1.908 || Validation Acc 0.53 f1 micro 0.53 f1 macro 0.529 loss 1.856 || \n",
            "Step 2: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCsL1sDQ44Hr"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  evaluate(\"Testing\", test_loader, model, criterion, False)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyTM03nJ-ETN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}