{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hate Speech.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNdMyv+drXqrIR0YuTs1VGY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipta007/hate-speech-election-2020/blob/main/Hate_Speech_distilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Uy6wwZckGg",
        "outputId": "7f1fbb35-20f0-44df-a546-d2486cb54144"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.6/dist-packages (from datasets) (0.0.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.4.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a40uLozp6ah7"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5HK17tCbCOs"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT64_J1RbuWq"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpwGOLY-b2YP"
      },
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "          \"mrm8488/distilroberta-finetuned-tweets-hate-speech\",\n",
        "          # num_labels = 2, # The number of output labels--2 for binary classification             # You can increase this for multi-class tasks.   \n",
        "          output_attentions = False, # Whether the model returns attentions weights.\n",
        "          output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "        )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl1KDq214TAW",
        "outputId": "a94e958b-676f-40e2-c94f-2f6d65f1e62b"
      },
      "source": [
        "model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Gc5Mt9cosO"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-tweets-hate-speech\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2V8b8xKfWWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d95fbe-c23f-4232-d182-b52292baad3c"
      },
      "source": [
        "for param in model.roberta.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "    print(param.requires_grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_nATxfc3Uqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b377fc50-0f94-44ee-8bb1-cd5ed681db75"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"hatexplain\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset hatexplain (/root/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/802fcd855438812094e336cea509c99b04b890e4e0846c0385877ee2c7361e93)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pdoCWCR3fsu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d241b739-886e-4e63-a2bb-6860743c009e"
      },
      "source": [
        "\" \".join(dataset[\"train\"][0][\"post_tokens\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'u really think i would not have been raped by feral hindu or muslim back in india or bangladesh and a neo nazi would rape me as well just to see me cry'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfb7_Sbf4kGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacc7afe-ae5c-4f92-ec95-c5ed854d1ba5"
      },
      "source": [
        "dataset[\"train\"][0][\"annotators\"][\"label\"]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEauMs_2epHq",
        "outputId": "f04547f7-c0c9-4541-e37b-53cf318fd34b"
      },
      "source": [
        "for i in range(40, 50):\n",
        "  print(dataset[\"train\"][i][\"annotators\"][\"label\"])\n",
        "  print(\" \".join(dataset[\"train\"][i][\"post_tokens\"]))\n",
        "  print(\"\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0]\n",
            "a nigger bich will never have strait hair so keep wearing dat horsehair weave\n",
            "\n",
            "[0, 0, 0]\n",
            "apparently some vile nigger animals kidnapped a white special needs teen and tortured and scalped him while shouting fuck white people fuck trump are you being armed and are you shooting in the head when these zombies approach\n",
            "\n",
            "[0, 0, 0]\n",
            "<user> you need potassium eat a banana you fucking nigger\n",
            "\n",
            "[2, 0, 0]\n",
            "well then send some coon estrogen to my wife she turning more liberal breath by breath due to fake news i do not know how much more i can take hell between her and my two daughters hormones i have started growing breast tissue like pseudo naturally becoming bruce jenner\n",
            "\n",
            "[0, 0, 0]\n",
            "it high time we hindus should think that these filthy muslims should be allowed to live here or not kamleshtiwari ypg yourprophetisgay\n",
            "\n",
            "[1, 1, 1]\n",
            "<user> why should you ban immigrants from operating small business every business start somewhere and tomorrow that will give people job\n",
            "\n",
            "[2, 0, 2]\n",
            "i am sure you are totally the alpha faggot at the standard fuckparties\n",
            "\n",
            "[2, 1, 2]\n",
            "<user> aint he bisexual an lgbt king of gays even his mind\n",
            "\n",
            "[2, 0, 2]\n",
            "you insecure lowlife broke ass desperate woman beating momma boy ass niggas are fucking sick get a grip on reality hoe dumbass dykes\n",
            "\n",
            "[0, 2, 0]\n",
            "i guess this nigger and his mudshark are unaware of the merchant this is some hilarious trolling\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow250cpu6lZs"
      },
      "source": [
        "def join_all(arr):\n",
        "  ret = []\n",
        "  for v in arr:\n",
        "    ret.append(\" \".join(v))\n",
        "  return ret\n",
        "\n",
        "train_encodings = tokenizer(join_all(dataset[\"train\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "val_encodings = tokenizer(join_all(dataset[\"validation\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(join_all(dataset[\"test\"][\"post_tokens\"]), padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyKvFyhl8A3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6457f010-ea4d-4b0f-a7da-617bc6efbb79"
      },
      "source": [
        "len(dataset[\"train\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15383"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EJ7cThpFBEr"
      },
      "source": [
        "class HateModel(torch.nn.Module):\n",
        "  def __init__(self, bert):\n",
        "    super(HateModel, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.linear1 = torch.nn.Linear(2, 128)\n",
        "    self.linear2 = torch.nn.Linear(128, 3)\n",
        "\n",
        "  def forward(self, x, attention_mask, labels):\n",
        "    x = self.bert(x, attention_mask=attention_mask, labels=labels)\n",
        "    print(x)\n",
        "    x = F.relu(self.linear1(x[1]))\n",
        "    x = F.softmax(self.linear2(x))\n",
        "    return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJVkda7qn9pR"
      },
      "source": [
        "class HateExplain(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, dataset):\n",
        "    self.dataset = dataset\n",
        "    self.encodings = encodings\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = self.get_label(self.dataset[idx][\"annotators\"][\"label\"])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def get_label(self, arr):\n",
        "    cnt = [0, 0, 0]\n",
        "    for v in arr:\n",
        "      cnt[v] += 1\n",
        "    \n",
        "    # 0 - hate\n",
        "    # 1 - normal\n",
        "    # 2 - offensive\n",
        "    return 1 if torch.argmax(torch.as_tensor(cnt)) != 1 else 0\n",
        "\n",
        "      \n",
        "\n",
        "train_dataset = HateExplain(train_encodings, dataset[\"train\"])\n",
        "val_dataset = HateExplain(val_encodings, dataset[\"validation\"])\n",
        "test_dataset = HateExplain(test_encodings, dataset[\"test\"])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKFy2s2MWOmb"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DEs4IBXvnh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe8b0ca-ebc4-4ebc-eb7f-381e477895d3"
      },
      "source": [
        "from transformers import AdamW\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "# hate_model = HateModel(model)\n",
        "# hate_model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "EPOCH = 400\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  model.train(True)\n",
        "  y_true = torch.as_tensor([]).to(device)\n",
        "  scores = torch.as_tensor([]).to(device)\n",
        "  cnt = 0\n",
        "  tot_loss = 0\n",
        "  for batch in train_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    # loss = criterion(y_pred, labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    y_pred = F.softmax(outputs[1], dim=1)\n",
        "    score = y_pred\n",
        "    scores = torch.cat([scores, score])\n",
        "    y_true = torch.cat([y_true, labels])\n",
        "\n",
        "    tot_loss += loss.item()\n",
        "    cnt += 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  y_pred = torch.argmax(scores, dim=1)\n",
        "  print(\"Training Epoch\", epoch + 1)\n",
        "  print(\"Micro f1:\", f1_score(y_true.cpu(), y_pred.cpu(), average=\"micro\"))\n",
        "  print(\"Macro f1:\", f1_score(y_true.cpu(), y_pred.cpu(), average=\"macro\"))\n",
        "  print(\"accuracy:\", accuracy_score(y_true.cpu(), y_pred.cpu()))\n",
        "  # print(\"ROC AUC score:\", roc_auc_score(y_true.cpu(), scores.detach().cpu(), average=\"weighted\", multi_class=\"ovr\"))\n",
        "  print(\"Loss:\", tot_loss / cnt)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.train(False)\n",
        "    y_true = torch.as_tensor([]).to(device)\n",
        "    scores = torch.as_tensor([]).to(device)\n",
        "    cnt = 0\n",
        "    tot_loss = 0\n",
        "    for batch in val_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      # loss = criterion(y_pred, labels)\n",
        "      loss = outputs[0]\n",
        "\n",
        "      y_pred = F.softmax(outputs[1], dim=1)\n",
        "      score = y_pred\n",
        "      scores = torch.cat([scores, score])\n",
        "      y_true = torch.cat([y_true, labels])\n",
        "      \n",
        "      tot_loss += loss.item()\n",
        "      cnt += 1\n",
        "  \n",
        "    y_pred = torch.argmax(scores, dim=1)\n",
        "    print(\"Validation Epoch\", epoch + 1)\n",
        "    print(\"Micro f1:\", f1_score(y_true.cpu(), y_pred.cpu(), average=\"micro\"))\n",
        "    print(\"Macro f1:\", f1_score(y_true.cpu(), y_pred.cpu(), average=\"macro\"))\n",
        "    print(\"accuracy:\", accuracy_score(y_true.cpu(), y_pred.cpu()))\n",
        "    # print(\"ROC AUC score:\", roc_auc_score(y_true.cpu(), scores.detach().cpu(), average=\"weighted\", multi_class=\"ovr\"))\n",
        "    print(\"Loss:\", tot_loss / cnt)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Training Epoch 1\n",
            "Micro f1: 0.6680751478905285\n",
            "Macro f1: 0.6342036356393609\n",
            "accuracy: 0.6680751478905285\n",
            "Loss: 0.6090345782016766\n",
            "Validation Epoch 1\n",
            "Micro f1: 0.6810613943808532\n",
            "Macro f1: 0.6531747019995826\n",
            "accuracy: 0.6810613943808532\n",
            "Loss: 0.5919122476715687\n",
            "\n",
            "\n",
            "Training Epoch 2\n",
            "Micro f1: 0.6828316973282195\n",
            "Macro f1: 0.6577286154390675\n",
            "accuracy: 0.6828316973282195\n",
            "Loss: 0.5902677984470637\n",
            "Validation Epoch 2\n",
            "Micro f1: 0.6966701352757544\n",
            "Macro f1: 0.6630640464151959\n",
            "accuracy: 0.6966701352757544\n",
            "Loss: 0.578581365918325\n",
            "\n",
            "\n",
            "Training Epoch 3\n",
            "Micro f1: 0.6874471819541051\n",
            "Macro f1: 0.6647938154758568\n",
            "accuracy: 0.6874471819541051\n",
            "Loss: 0.5871233382802495\n",
            "Validation Epoch 3\n",
            "Micro f1: 0.7065556711758585\n",
            "Macro f1: 0.6888896542223446\n",
            "accuracy: 0.7065556711758585\n",
            "Loss: 0.5709110000901971\n",
            "\n",
            "\n",
            "Training Epoch 4\n",
            "Micro f1: 0.6895274003770396\n",
            "Macro f1: 0.666391894497077\n",
            "accuracy: 0.6895274003770396\n",
            "Loss: 0.5844808244407796\n",
            "Validation Epoch 4\n",
            "Micro f1: 0.696149843912591\n",
            "Macro f1: 0.6906619397070123\n",
            "accuracy: 0.696149843912591\n",
            "Loss: 0.5777323328512759\n",
            "\n",
            "\n",
            "Training Epoch 5\n",
            "Micro f1: 0.6917376324514074\n",
            "Macro f1: 0.6693472650801395\n",
            "accuracy: 0.6917376324514074\n",
            "Loss: 0.5835894538310362\n",
            "Validation Epoch 5\n",
            "Micro f1: 0.7029136316337149\n",
            "Macro f1: 0.6893931224869335\n",
            "accuracy: 0.7029136316337149\n",
            "Loss: 0.5717278654910316\n",
            "\n",
            "\n",
            "Training Epoch 6\n",
            "Micro f1: 0.6905675095885068\n",
            "Macro f1: 0.667753369643159\n",
            "accuracy: 0.6905675095885068\n",
            "Loss: 0.5832910810649519\n",
            "Validation Epoch 6\n",
            "Micro f1: 0.7018730489073881\n",
            "Macro f1: 0.6725674372721653\n",
            "accuracy: 0.7018730489073881\n",
            "Loss: 0.5740727267482064\n",
            "\n",
            "\n",
            "Training Epoch 7\n",
            "Micro f1: 0.6923877007085744\n",
            "Macro f1: 0.6701163252102731\n",
            "accuracy: 0.6923877007085744\n",
            "Loss: 0.5774746722621135\n",
            "Validation Epoch 7\n",
            "Micro f1: 0.7008324661810614\n",
            "Macro f1: 0.6748827309674088\n",
            "accuracy: 0.7008324661810614\n",
            "Loss: 0.5726044005598904\n",
            "\n",
            "\n",
            "Training Epoch 8\n",
            "Micro f1: 0.6916726256256907\n",
            "Macro f1: 0.6688065136949715\n",
            "accuracy: 0.6916726256256907\n",
            "Loss: 0.5828556432049884\n",
            "Validation Epoch 8\n",
            "Micro f1: 0.7096774193548389\n",
            "Macro f1: 0.6880567112501048\n",
            "accuracy: 0.7096774193548387\n",
            "Loss: 0.5673760377671108\n",
            "\n",
            "\n",
            "Training Epoch 9\n",
            "Micro f1: 0.6874471819541051\n",
            "Macro f1: 0.6656110384295191\n",
            "accuracy: 0.6874471819541051\n",
            "Loss: 0.5794210097212306\n",
            "Validation Epoch 9\n",
            "Micro f1: 0.696149843912591\n",
            "Macro f1: 0.6603488114532358\n",
            "accuracy: 0.696149843912591\n",
            "Loss: 0.5722847558742712\n",
            "\n",
            "\n",
            "Training Epoch 10\n",
            "Micro f1: 0.6942728986543587\n",
            "Macro f1: 0.6720547755577236\n",
            "accuracy: 0.6942728986543587\n",
            "Loss: 0.5782880815919372\n",
            "Validation Epoch 10\n",
            "Micro f1: 0.686264308012487\n",
            "Macro f1: 0.6561261339046732\n",
            "accuracy: 0.686264308012487\n",
            "Loss: 0.5827872982202482\n",
            "\n",
            "\n",
            "Training Epoch 11\n",
            "Micro f1: 0.6946629396086589\n",
            "Macro f1: 0.672757403092042\n",
            "accuracy: 0.6946629396086589\n",
            "Loss: 0.5776876143940769\n",
            "Validation Epoch 11\n",
            "Micro f1: 0.7034339229968782\n",
            "Macro f1: 0.6867153872462501\n",
            "accuracy: 0.7034339229968782\n",
            "Loss: 0.572194420848011\n",
            "\n",
            "\n",
            "Training Epoch 12\n",
            "Micro f1: 0.6936878372229084\n",
            "Macro f1: 0.6717478331688556\n",
            "accuracy: 0.6936878372229084\n",
            "Loss: 0.5800055679313358\n",
            "Validation Epoch 12\n",
            "Micro f1: 0.7065556711758585\n",
            "Macro f1: 0.68561872909699\n",
            "accuracy: 0.7065556711758585\n",
            "Loss: 0.5706411863654113\n",
            "\n",
            "\n",
            "Training Epoch 13\n",
            "Micro f1: 0.6959630761229929\n",
            "Macro f1: 0.6748257917614867\n",
            "accuracy: 0.6959630761229929\n",
            "Loss: 0.5784529066507137\n",
            "Validation Epoch 13\n",
            "Micro f1: 0.6956295525494277\n",
            "Macro f1: 0.6690166405705642\n",
            "accuracy: 0.6956295525494277\n",
            "Loss: 0.575255969585466\n",
            "\n",
            "\n",
            "Training Epoch 14\n",
            "Micro f1: 0.6951829942143926\n",
            "Macro f1: 0.6741068664676196\n",
            "accuracy: 0.6951829942143926\n",
            "Loss: 0.5758523108309866\n",
            "Validation Epoch 14\n",
            "Micro f1: 0.7112382934443289\n",
            "Macro f1: 0.6932854379662889\n",
            "accuracy: 0.7112382934443289\n",
            "Loss: 0.5670648340351325\n",
            "\n",
            "\n",
            "Training Epoch 15\n",
            "Micro f1: 0.7004485470974452\n",
            "Macro f1: 0.6786908893904013\n",
            "accuracy: 0.7004485470974452\n",
            "Loss: 0.5734101408732408\n",
            "Validation Epoch 15\n",
            "Micro f1: 0.7091571279916753\n",
            "Macro f1: 0.6831135858814217\n",
            "accuracy: 0.7091571279916753\n",
            "Loss: 0.5765560333393822\n",
            "\n",
            "\n",
            "Training Epoch 16\n",
            "Micro f1: 0.6958980692972763\n",
            "Macro f1: 0.6725580249538107\n",
            "accuracy: 0.6958980692972763\n",
            "Loss: 0.5769389655882504\n",
            "Validation Epoch 16\n",
            "Micro f1: 0.7060353798126952\n",
            "Macro f1: 0.6919489964010175\n",
            "accuracy: 0.7060353798126952\n",
            "Loss: 0.5639751358958315\n",
            "\n",
            "\n",
            "Training Epoch 17\n",
            "Micro f1: 0.701813690437496\n",
            "Macro f1: 0.6809017792319225\n",
            "accuracy: 0.701813690437496\n",
            "Loss: 0.5744430887166279\n",
            "Validation Epoch 17\n",
            "Micro f1: 0.7133194588969823\n",
            "Macro f1: 0.6956584088064683\n",
            "accuracy: 0.7133194588969823\n",
            "Loss: 0.5658372842575893\n",
            "\n",
            "\n",
            "Training Epoch 18\n",
            "Micro f1: 0.6942728986543587\n",
            "Macro f1: 0.6722684353727388\n",
            "accuracy: 0.6942728986543587\n",
            "Loss: 0.5757861604871473\n",
            "Validation Epoch 18\n",
            "Micro f1: 0.7091571279916753\n",
            "Macro f1: 0.6904068892087408\n",
            "accuracy: 0.7091571279916753\n",
            "Loss: 0.5629125968976454\n",
            "\n",
            "\n",
            "Training Epoch 19\n",
            "Micro f1: 0.6947279464343756\n",
            "Macro f1: 0.6730973517645401\n",
            "accuracy: 0.6947279464343756\n",
            "Loss: 0.5731108955372891\n",
            "Validation Epoch 19\n",
            "Micro f1: 0.7023933402705516\n",
            "Macro f1: 0.6800414432977689\n",
            "accuracy: 0.7023933402705516\n",
            "Loss: 0.5689032053159289\n",
            "\n",
            "\n",
            "Training Epoch 20\n",
            "Micro f1: 0.7014236494831957\n",
            "Macro f1: 0.6803246978752469\n",
            "accuracy: 0.7014236494831957\n",
            "Loss: 0.5724885753683142\n",
            "Validation Epoch 20\n",
            "Micro f1: 0.7023933402705516\n",
            "Macro f1: 0.694549374586716\n",
            "accuracy: 0.7023933402705516\n",
            "Loss: 0.5723325124949463\n",
            "\n",
            "\n",
            "Training Epoch 21\n",
            "Micro f1: 0.6990834037573945\n",
            "Macro f1: 0.6780716530124\n",
            "accuracy: 0.6990834037573945\n",
            "Loss: 0.5731795510730228\n",
            "Validation Epoch 21\n",
            "Micro f1: 0.7133194588969823\n",
            "Macro f1: 0.6915137157027327\n",
            "accuracy: 0.7133194588969823\n",
            "Loss: 0.5651648278571357\n",
            "\n",
            "\n",
            "Training Epoch 22\n",
            "Micro f1: 0.6983033218487942\n",
            "Macro f1: 0.676284000295728\n",
            "accuracy: 0.6983033218487942\n",
            "Loss: 0.572949224877754\n",
            "Validation Epoch 22\n",
            "Micro f1: 0.7018730489073881\n",
            "Macro f1: 0.6874410922048012\n",
            "accuracy: 0.7018730489073881\n",
            "Loss: 0.5647226032639338\n",
            "\n",
            "\n",
            "Training Epoch 23\n",
            "Micro f1: 0.6975232399401937\n",
            "Macro f1: 0.6763795440523821\n",
            "accuracy: 0.6975232399401937\n",
            "Loss: 0.5725180755471985\n",
            "Validation Epoch 23\n",
            "Micro f1: 0.7081165452653486\n",
            "Macro f1: 0.6798609756466887\n",
            "accuracy: 0.7081165452653486\n",
            "Loss: 0.5698432727786135\n",
            "\n",
            "\n",
            "Training Epoch 24\n",
            "Micro f1: 0.6980432945459273\n",
            "Macro f1: 0.6762634139838034\n",
            "accuracy: 0.6980432945459273\n",
            "Loss: 0.5738564784524347\n",
            "Validation Epoch 24\n",
            "Micro f1: 0.7065556711758585\n",
            "Macro f1: 0.6898552515930565\n",
            "accuracy: 0.7065556711758585\n",
            "Loss: 0.5652901769176988\n",
            "\n",
            "\n",
            "Training Epoch 25\n",
            "Micro f1: 0.7003835402717286\n",
            "Macro f1: 0.6796216727420172\n",
            "accuracy: 0.7003835402717286\n",
            "Loss: 0.5707499167093864\n",
            "Validation Epoch 25\n",
            "Micro f1: 0.7091571279916753\n",
            "Macro f1: 0.6929943196413992\n",
            "accuracy: 0.7091571279916753\n",
            "Loss: 0.5637888839422178\n",
            "\n",
            "\n",
            "Training Epoch 26\n",
            "Micro f1: 0.6996684651888448\n",
            "Macro f1: 0.6789591856315729\n",
            "accuracy: 0.6996684651888448\n",
            "Loss: 0.5706975603177988\n",
            "Validation Epoch 26\n",
            "Micro f1: 0.7023933402705516\n",
            "Macro f1: 0.670259057660215\n",
            "accuracy: 0.7023933402705516\n",
            "Loss: 0.5716865057787619\n",
            "\n",
            "\n",
            "Training Epoch 27\n",
            "Micro f1: 0.6993434310602613\n",
            "Macro f1: 0.6774000218417888\n",
            "accuracy: 0.6993434310602613\n",
            "Loss: 0.5724590137569919\n",
            "Validation Epoch 27\n",
            "Micro f1: 0.7075962539021851\n",
            "Macro f1: 0.6924894551284971\n",
            "accuracy: 0.7075962539021852\n",
            "Loss: 0.5619119615101618\n",
            "\n",
            "\n",
            "Training Epoch 28\n",
            "Micro f1: 0.7018786972632126\n",
            "Macro f1: 0.6814341272759983\n",
            "accuracy: 0.7018786972632126\n",
            "Loss: 0.5707808030989958\n",
            "Validation Epoch 28\n",
            "Micro f1: 0.7112382934443289\n",
            "Macro f1: 0.6944122396677097\n",
            "accuracy: 0.7112382934443289\n",
            "Loss: 0.563692287472654\n",
            "\n",
            "\n",
            "Training Epoch 29\n",
            "Micro f1: 0.7005785607488786\n",
            "Macro f1: 0.6798868159705671\n",
            "accuracy: 0.7005785607488786\n",
            "Loss: 0.5678406615515013\n",
            "Validation Epoch 29\n",
            "Micro f1: 0.7138397502601457\n",
            "Macro f1: 0.6956395054221142\n",
            "accuracy: 0.7138397502601457\n",
            "Loss: 0.5660621225833893\n",
            "\n",
            "\n",
            "Training Epoch 30\n",
            "Micro f1: 0.7021387245660794\n",
            "Macro f1: 0.6811472814269277\n",
            "accuracy: 0.7021387245660794\n",
            "Loss: 0.5704001205936539\n",
            "Validation Epoch 30\n",
            "Micro f1: 0.7107180020811654\n",
            "Macro f1: 0.6867155664221679\n",
            "accuracy: 0.7107180020811654\n",
            "Loss: 0.5644185168684022\n",
            "\n",
            "\n",
            "Training Epoch 31\n",
            "Micro f1: 0.70330884742898\n",
            "Macro f1: 0.682149461005113\n",
            "accuracy: 0.70330884742898\n",
            "Loss: 0.5664540808686596\n",
            "Validation Epoch 31\n",
            "Micro f1: 0.7039542143600416\n",
            "Macro f1: 0.6871854851419842\n",
            "accuracy: 0.7039542143600416\n",
            "Loss: 0.5634752398680064\n",
            "\n",
            "\n",
            "Training Epoch 32\n",
            "Micro f1: 0.700513553923162\n",
            "Macro f1: 0.6795788866213892\n",
            "accuracy: 0.700513553923162\n",
            "Loss: 0.5672782500605573\n",
            "Validation Epoch 32\n",
            "Micro f1: 0.7070759625390218\n",
            "Macro f1: 0.693465180043733\n",
            "accuracy: 0.7070759625390218\n",
            "Loss: 0.5632246594783689\n",
            "\n",
            "\n",
            "Training Epoch 33\n",
            "Micro f1: 0.7026587791718131\n",
            "Macro f1: 0.6813842778867858\n",
            "accuracy: 0.7026587791718131\n",
            "Loss: 0.5678711143204179\n",
            "Validation Epoch 33\n",
            "Micro f1: 0.7096774193548389\n",
            "Macro f1: 0.6851943825092727\n",
            "accuracy: 0.7096774193548387\n",
            "Loss: 0.5659292729432918\n",
            "\n",
            "\n",
            "Training Epoch 34\n",
            "Micro f1: 0.703958915686147\n",
            "Macro f1: 0.6829142106243695\n",
            "accuracy: 0.703958915686147\n",
            "Loss: 0.5698537951695448\n",
            "Validation Epoch 34\n",
            "Micro f1: 0.7096774193548389\n",
            "Macro f1: 0.6882398836302073\n",
            "accuracy: 0.7096774193548387\n",
            "Loss: 0.5639422311763133\n",
            "\n",
            "\n",
            "Training Epoch 35\n",
            "Micro f1: 0.7020737177403628\n",
            "Macro f1: 0.6809750446896697\n",
            "accuracy: 0.7020737177403628\n",
            "Loss: 0.5680263855043419\n",
            "Validation Epoch 35\n",
            "Micro f1: 0.7086368366285121\n",
            "Macro f1: 0.6867538605712423\n",
            "accuracy: 0.708636836628512\n",
            "Loss: 0.5664149953806696\n",
            "\n",
            "\n",
            "Training Epoch 36\n",
            "Micro f1: 0.707014236494832\n",
            "Macro f1: 0.6868887787957667\n",
            "accuracy: 0.707014236494832\n",
            "Loss: 0.5632622082739015\n",
            "Validation Epoch 36\n",
            "Micro f1: 0.7086368366285121\n",
            "Macro f1: 0.6883830382537962\n",
            "accuracy: 0.708636836628512\n",
            "Loss: 0.5590229512246188\n",
            "\n",
            "\n",
            "Training Epoch 37\n",
            "Micro f1: 0.7051940453747644\n",
            "Macro f1: 0.6850543579483133\n",
            "accuracy: 0.7051940453747644\n",
            "Loss: 0.5646392996432628\n",
            "Validation Epoch 37\n",
            "Micro f1: 0.7107180020811654\n",
            "Macro f1: 0.6963561556646664\n",
            "accuracy: 0.7107180020811654\n",
            "Loss: 0.5608268853061456\n",
            "\n",
            "\n",
            "Training Epoch 38\n",
            "Micro f1: 0.7062991614119483\n",
            "Macro f1: 0.6858247365824869\n",
            "accuracy: 0.7062991614119483\n",
            "Loss: 0.5654962217888316\n",
            "Validation Epoch 38\n",
            "Micro f1: 0.7112382934443289\n",
            "Macro f1: 0.6983691387610544\n",
            "accuracy: 0.7112382934443289\n",
            "Loss: 0.5619344891102847\n",
            "\n",
            "\n",
            "Training Epoch 39\n",
            "Micro f1: 0.7026587791718131\n",
            "Macro f1: 0.6825572213977468\n",
            "accuracy: 0.7026587791718131\n",
            "Loss: 0.5657177756767016\n",
            "Validation Epoch 39\n",
            "Micro f1: 0.7138397502601457\n",
            "Macro f1: 0.6984649122807018\n",
            "accuracy: 0.7138397502601457\n",
            "Loss: 0.5663153255773969\n",
            "\n",
            "\n",
            "Training Epoch 40\n",
            "Micro f1: 0.7073392706234155\n",
            "Macro f1: 0.6871589293751821\n",
            "accuracy: 0.7073392706234155\n",
            "Loss: 0.5647707456052923\n",
            "Validation Epoch 40\n",
            "Micro f1: 0.6987513007284079\n",
            "Macro f1: 0.6610511138056343\n",
            "accuracy: 0.6987513007284079\n",
            "Loss: 0.5714566766723128\n",
            "\n",
            "\n",
            "Training Epoch 41\n",
            "Micro f1: 0.7036338815575636\n",
            "Macro f1: 0.682577456983706\n",
            "accuracy: 0.7036338815575636\n",
            "Loss: 0.5658383220743984\n",
            "Validation Epoch 41\n",
            "Micro f1: 0.7112382934443289\n",
            "Macro f1: 0.6896423807147731\n",
            "accuracy: 0.7112382934443289\n",
            "Loss: 0.5561108686460936\n",
            "\n",
            "\n",
            "Training Epoch 42\n",
            "Micro f1: 0.708314373009166\n",
            "Macro f1: 0.6879249976709731\n",
            "accuracy: 0.708314373009166\n",
            "Loss: 0.5622409683441174\n",
            "Validation Epoch 42\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.6981605628242644\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.5579705248194292\n",
            "\n",
            "\n",
            "Training Epoch 43\n",
            "Micro f1: 0.7092244685691997\n",
            "Macro f1: 0.6890311875536078\n",
            "accuracy: 0.7092244685691997\n",
            "Loss: 0.5626185733104694\n",
            "Validation Epoch 43\n",
            "Micro f1: 0.7148803329864724\n",
            "Macro f1: 0.6925482228760917\n",
            "accuracy: 0.7148803329864725\n",
            "Loss: 0.5567183430529823\n",
            "\n",
            "\n",
            "Training Epoch 44\n",
            "Micro f1: 0.7072092569719821\n",
            "Macro f1: 0.6869757314657912\n",
            "accuracy: 0.707209256971982\n",
            "Loss: 0.5628690780634196\n",
            "Validation Epoch 44\n",
            "Micro f1: 0.7112382934443289\n",
            "Macro f1: 0.6857881365293272\n",
            "accuracy: 0.7112382934443289\n",
            "Loss: 0.5614163715484713\n",
            "\n",
            "\n",
            "Training Epoch 45\n",
            "Micro f1: 0.7043489566404473\n",
            "Macro f1: 0.6835584548949963\n",
            "accuracy: 0.7043489566404473\n",
            "Loss: 0.5630979927686545\n",
            "Validation Epoch 45\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.7016062840397855\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.556712746373878\n",
            "\n",
            "\n",
            "Training Epoch 46\n",
            "Micro f1: 0.7077293115777157\n",
            "Macro f1: 0.6879705438137887\n",
            "accuracy: 0.7077293115777157\n",
            "Loss: 0.5591848044967949\n",
            "Validation Epoch 46\n",
            "Micro f1: 0.7138397502601457\n",
            "Macro f1: 0.6937735007329042\n",
            "accuracy: 0.7138397502601457\n",
            "Loss: 0.5599492214435389\n",
            "\n",
            "\n",
            "Training Epoch 47\n",
            "Micro f1: 0.7098745368263668\n",
            "Macro f1: 0.6904442020547681\n",
            "accuracy: 0.7098745368263668\n",
            "Loss: 0.5589210861314111\n",
            "Validation Epoch 47\n",
            "Micro f1: 0.7127991675338188\n",
            "Macro f1: 0.6906767801164353\n",
            "accuracy: 0.7127991675338189\n",
            "Loss: 0.5600647219449035\n",
            "\n",
            "\n",
            "Training Epoch 48\n",
            "Micro f1: 0.7040239225118637\n",
            "Macro f1: 0.6838042980680639\n",
            "accuracy: 0.7040239225118637\n",
            "Loss: 0.5635219633021622\n",
            "Validation Epoch 48\n",
            "Micro f1: 0.7039542143600416\n",
            "Macro f1: 0.6772345810183648\n",
            "accuracy: 0.7039542143600416\n",
            "Loss: 0.5621863881911128\n",
            "\n",
            "\n",
            "Training Epoch 49\n",
            "Micro f1: 0.711759734772151\n",
            "Macro f1: 0.6924449654674464\n",
            "accuracy: 0.711759734772151\n",
            "Loss: 0.5589287119323152\n",
            "Validation Epoch 49\n",
            "Micro f1: 0.7159209157127991\n",
            "Macro f1: 0.6988187078109933\n",
            "accuracy: 0.7159209157127991\n",
            "Loss: 0.5570090783775345\n",
            "\n",
            "\n",
            "Training Epoch 50\n",
            "Micro f1: 0.7066241955405318\n",
            "Macro f1: 0.6862947730711053\n",
            "accuracy: 0.7066241955405318\n",
            "Loss: 0.5607799950855438\n",
            "Validation Epoch 50\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.697628147586041\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.5595173833291393\n",
            "\n",
            "\n",
            "Training Epoch 51\n",
            "Micro f1: 0.7062341545862315\n",
            "Macro f1: 0.6867771694608671\n",
            "accuracy: 0.7062341545862315\n",
            "Loss: 0.5584673175197133\n",
            "Validation Epoch 51\n",
            "Micro f1: 0.714360041623309\n",
            "Macro f1: 0.6917066722997403\n",
            "accuracy: 0.714360041623309\n",
            "Loss: 0.5540658900560427\n",
            "\n",
            "\n",
            "Training Epoch 52\n",
            "Micro f1: 0.7112396801664175\n",
            "Macro f1: 0.6911096679724229\n",
            "accuracy: 0.7112396801664175\n",
            "Loss: 0.5572742011839535\n",
            "Validation Epoch 52\n",
            "Micro f1: 0.7127991675338188\n",
            "Macro f1: 0.7011159303194314\n",
            "accuracy: 0.7127991675338189\n",
            "Loss: 0.5600936018730983\n",
            "\n",
            "\n",
            "Training Epoch 53\n",
            "Micro f1: 0.7114347006435676\n",
            "Macro f1: 0.6922156839802738\n",
            "accuracy: 0.7114347006435676\n",
            "Loss: 0.5591459267786287\n",
            "Validation Epoch 53\n",
            "Micro f1: 0.7091571279916753\n",
            "Macro f1: 0.6805675347908209\n",
            "accuracy: 0.7091571279916753\n",
            "Loss: 0.559577228116595\n",
            "\n",
            "\n",
            "Training Epoch 54\n",
            "Micro f1: 0.7090294480920497\n",
            "Macro f1: 0.6888777065584506\n",
            "accuracy: 0.7090294480920497\n",
            "Loss: 0.559463059641963\n",
            "Validation Epoch 54\n",
            "Micro f1: 0.7117585848074922\n",
            "Macro f1: 0.6801735396489559\n",
            "accuracy: 0.7117585848074922\n",
            "Loss: 0.5594945589372934\n",
            "\n",
            "\n",
            "Training Epoch 55\n",
            "Micro f1: 0.7113046869921342\n",
            "Macro f1: 0.6911023687055129\n",
            "accuracy: 0.7113046869921342\n",
            "Loss: 0.5560062349833966\n",
            "Validation Epoch 55\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7010541920365823\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.552822374854206\n",
            "\n",
            "\n",
            "Training Epoch 56\n",
            "Micro f1: 0.710914646037834\n",
            "Macro f1: 0.691187449340104\n",
            "accuracy: 0.7109146460378339\n",
            "Loss: 0.5538936288961501\n",
            "Validation Epoch 56\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.6980552154636689\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.5538347274803919\n",
            "\n",
            "\n",
            "Training Epoch 57\n",
            "Micro f1: 0.7102645777806671\n",
            "Macro f1: 0.6904059897658279\n",
            "accuracy: 0.710264577780667\n",
            "Loss: 0.5566981126885404\n",
            "Validation Epoch 57\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.7024825371026102\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.5568549660119143\n",
            "\n",
            "\n",
            "Training Epoch 58\n",
            "Micro f1: 0.7120197620750178\n",
            "Macro f1: 0.6932101032391167\n",
            "accuracy: 0.7120197620750178\n",
            "Loss: 0.5560699292069413\n",
            "Validation Epoch 58\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7014736268952615\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5565139587260475\n",
            "\n",
            "\n",
            "Training Epoch 59\n",
            "Micro f1: 0.7105896119092505\n",
            "Macro f1: 0.6903922921251665\n",
            "accuracy: 0.7105896119092505\n",
            "Loss: 0.5569310196348138\n",
            "Validation Epoch 59\n",
            "Micro f1: 0.7206035379812695\n",
            "Macro f1: 0.7033912260055779\n",
            "accuracy: 0.7206035379812695\n",
            "Loss: 0.5551193930393408\n",
            "\n",
            "\n",
            "Training Epoch 60\n",
            "Micro f1: 0.7157251511408698\n",
            "Macro f1: 0.6965810558120045\n",
            "accuracy: 0.7157251511408698\n",
            "Loss: 0.5557997130988294\n",
            "Validation Epoch 60\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.6946040233827608\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.555492664910545\n",
            "\n",
            "\n",
            "Training Epoch 61\n",
            "Micro f1: 0.7168302671780538\n",
            "Macro f1: 0.6978762716697173\n",
            "accuracy: 0.7168302671780536\n",
            "Loss: 0.5538781497805629\n",
            "Validation Epoch 61\n",
            "Micro f1: 0.7117585848074922\n",
            "Macro f1: 0.7010356054658764\n",
            "accuracy: 0.7117585848074922\n",
            "Loss: 0.5619610717966537\n",
            "\n",
            "\n",
            "Training Epoch 62\n",
            "Micro f1: 0.7137749463693687\n",
            "Macro f1: 0.6945420844773516\n",
            "accuracy: 0.7137749463693688\n",
            "Loss: 0.5543209494027675\n",
            "Validation Epoch 62\n",
            "Micro f1: 0.7138397502601457\n",
            "Macro f1: 0.6897077246955197\n",
            "accuracy: 0.7138397502601457\n",
            "Loss: 0.5546521505048453\n",
            "\n",
            "\n",
            "Training Epoch 63\n",
            "Micro f1: 0.7090294480920497\n",
            "Macro f1: 0.6896394807321313\n",
            "accuracy: 0.7090294480920497\n",
            "Loss: 0.5563173112837044\n",
            "Validation Epoch 63\n",
            "Micro f1: 0.7096774193548389\n",
            "Macro f1: 0.6923607374098864\n",
            "accuracy: 0.7096774193548387\n",
            "Loss: 0.5622372646962316\n",
            "\n",
            "\n",
            "Training Epoch 64\n",
            "Micro f1: 0.7144900214522525\n",
            "Macro f1: 0.6956323926419106\n",
            "accuracy: 0.7144900214522525\n",
            "Loss: 0.5529040199803216\n",
            "Validation Epoch 64\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7026411264982586\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5527030458627653\n",
            "\n",
            "\n",
            "Training Epoch 65\n",
            "Micro f1: 0.7119547552493012\n",
            "Macro f1: 0.6928554330849337\n",
            "accuracy: 0.7119547552493012\n",
            "Loss: 0.5534077619750386\n",
            "Validation Epoch 65\n",
            "Micro f1: 0.7127991675338188\n",
            "Macro f1: 0.6912288054202247\n",
            "accuracy: 0.7127991675338189\n",
            "Loss: 0.5552668864569388\n",
            "\n",
            "\n",
            "Training Epoch 66\n",
            "Micro f1: 0.7127998439836182\n",
            "Macro f1: 0.694145893350177\n",
            "accuracy: 0.7127998439836183\n",
            "Loss: 0.5534922155917559\n",
            "Validation Epoch 66\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7009423241665065\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5515261605258815\n",
            "\n",
            "\n",
            "Training Epoch 67\n",
            "Micro f1: 0.7165702398751868\n",
            "Macro f1: 0.6977455954149326\n",
            "accuracy: 0.7165702398751869\n",
            "Loss: 0.5525714745144834\n",
            "Validation Epoch 67\n",
            "Micro f1: 0.7190426638917794\n",
            "Macro f1: 0.7101855585353241\n",
            "accuracy: 0.7190426638917794\n",
            "Loss: 0.5581137774404415\n",
            "\n",
            "\n",
            "Training Epoch 68\n",
            "Micro f1: 0.7183904309952546\n",
            "Macro f1: 0.6999556951777801\n",
            "accuracy: 0.7183904309952545\n",
            "Loss: 0.5508060009526612\n",
            "Validation Epoch 68\n",
            "Micro f1: 0.7195629552549427\n",
            "Macro f1: 0.6922063548891705\n",
            "accuracy: 0.7195629552549427\n",
            "Loss: 0.5545616024289249\n",
            "\n",
            "\n",
            "Training Epoch 69\n",
            "Micro f1: 0.7150750828837028\n",
            "Macro f1: 0.6959295835258305\n",
            "accuracy: 0.7150750828837028\n",
            "Loss: 0.5517996993853\n",
            "Validation Epoch 69\n",
            "Micro f1: 0.7174817898022893\n",
            "Macro f1: 0.6995944012344986\n",
            "accuracy: 0.7174817898022893\n",
            "Loss: 0.5560331246084418\n",
            "\n",
            "\n",
            "Training Epoch 70\n",
            "Micro f1: 0.7147500487551193\n",
            "Macro f1: 0.6956144888732083\n",
            "accuracy: 0.7147500487551193\n",
            "Loss: 0.5514928692034029\n",
            "Validation Epoch 70\n",
            "Micro f1: 0.7211238293444329\n",
            "Macro f1: 0.7114453612983099\n",
            "accuracy: 0.7211238293444329\n",
            "Loss: 0.5568588213487105\n",
            "\n",
            "\n",
            "Training Epoch 71\n",
            "Micro f1: 0.7181304036923877\n",
            "Macro f1: 0.6999864702940193\n",
            "accuracy: 0.7181304036923877\n",
            "Loss: 0.5462627259088901\n",
            "Validation Epoch 71\n",
            "Micro f1: 0.714360041623309\n",
            "Macro f1: 0.6970841444415131\n",
            "accuracy: 0.714360041623309\n",
            "Loss: 0.5541435977644171\n",
            "\n",
            "\n",
            "Training Epoch 72\n",
            "Micro f1: 0.7157901579665865\n",
            "Macro f1: 0.6969765635419257\n",
            "accuracy: 0.7157901579665865\n",
            "Loss: 0.5479122365741621\n",
            "Validation Epoch 72\n",
            "Micro f1: 0.7148803329864724\n",
            "Macro f1: 0.6929182527242872\n",
            "accuracy: 0.7148803329864725\n",
            "Loss: 0.553481064552118\n",
            "\n",
            "\n",
            "Training Epoch 73\n",
            "Micro f1: 0.7113696938178509\n",
            "Macro f1: 0.6927905738250566\n",
            "accuracy: 0.7113696938178509\n",
            "Loss: 0.552288649328781\n",
            "Validation Epoch 73\n",
            "Micro f1: 0.7206035379812695\n",
            "Macro f1: 0.7038610021992778\n",
            "accuracy: 0.7206035379812695\n",
            "Loss: 0.5513662828886805\n",
            "\n",
            "\n",
            "Training Epoch 74\n",
            "Micro f1: 0.7142299941493856\n",
            "Macro f1: 0.6949320064596922\n",
            "accuracy: 0.7142299941493857\n",
            "Loss: 0.5494625851233139\n",
            "Validation Epoch 74\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.6975323482237948\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.5532841704601099\n",
            "\n",
            "\n",
            "Training Epoch 75\n",
            "Micro f1: 0.71631021257232\n",
            "Macro f1: 0.6977185375608944\n",
            "accuracy: 0.7163102125723201\n",
            "Loss: 0.5486449545981235\n",
            "Validation Epoch 75\n",
            "Micro f1: 0.7185223725286161\n",
            "Macro f1: 0.7079912459325124\n",
            "accuracy: 0.7185223725286161\n",
            "Loss: 0.5579536431584476\n",
            "\n",
            "\n",
            "Training Epoch 76\n",
            "Micro f1: 0.7193655333810048\n",
            "Macro f1: 0.7013112686089815\n",
            "accuracy: 0.719365533381005\n",
            "Loss: 0.5487651062234772\n",
            "Validation Epoch 76\n",
            "Micro f1: 0.7190426638917794\n",
            "Macro f1: 0.6953494024283284\n",
            "accuracy: 0.7190426638917794\n",
            "Loss: 0.5545606682123232\n",
            "\n",
            "\n",
            "Training Epoch 77\n",
            "Micro f1: 0.7187154651238381\n",
            "Macro f1: 0.7000021475986163\n",
            "accuracy: 0.718715465123838\n",
            "Loss: 0.5479078895089037\n",
            "Validation Epoch 77\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7080012302174001\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5527616456027858\n",
            "\n",
            "\n",
            "Training Epoch 78\n",
            "Micro f1: 0.7210557108496392\n",
            "Macro f1: 0.7027443830217356\n",
            "accuracy: 0.7210557108496393\n",
            "Loss: 0.5464204497124202\n",
            "Validation Epoch 78\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.7022926949745674\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.5523460046811537\n",
            "\n",
            "\n",
            "Training Epoch 79\n",
            "Micro f1: 0.7195605538581552\n",
            "Macro f1: 0.7010378282917127\n",
            "accuracy: 0.7195605538581551\n",
            "Loss: 0.5469212504471662\n",
            "Validation Epoch 79\n",
            "Micro f1: 0.7018730489073881\n",
            "Macro f1: 0.6851463834057352\n",
            "accuracy: 0.7018730489073881\n",
            "Loss: 0.560714891380515\n",
            "\n",
            "\n",
            "Training Epoch 80\n",
            "Micro f1: 0.7200806084638887\n",
            "Macro f1: 0.7023648709369437\n",
            "accuracy: 0.7200806084638887\n",
            "Loss: 0.5449600144446268\n",
            "Validation Epoch 80\n",
            "Micro f1: 0.7206035379812695\n",
            "Macro f1: 0.7115229653505237\n",
            "accuracy: 0.7206035379812695\n",
            "Loss: 0.5559933146169364\n",
            "\n",
            "\n",
            "Training Epoch 81\n",
            "Micro f1: 0.7154001170122863\n",
            "Macro f1: 0.6968735748774788\n",
            "accuracy: 0.7154001170122863\n",
            "Loss: 0.5489685998884903\n",
            "Validation Epoch 81\n",
            "Micro f1: 0.7159209157127991\n",
            "Macro f1: 0.7080285390910093\n",
            "accuracy: 0.7159209157127991\n",
            "Loss: 0.5584658857219476\n",
            "\n",
            "\n",
            "Training Epoch 82\n",
            "Micro f1: 0.7165702398751868\n",
            "Macro f1: 0.698161180399903\n",
            "accuracy: 0.7165702398751869\n",
            "Loss: 0.5478183925151825\n",
            "Validation Epoch 82\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.6924316920235545\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.5582950856567415\n",
            "\n",
            "\n",
            "Training Epoch 83\n",
            "Micro f1: 0.7187804719495547\n",
            "Macro f1: 0.7003506859506026\n",
            "accuracy: 0.7187804719495547\n",
            "Loss: 0.5453078994012425\n",
            "Validation Epoch 83\n",
            "Micro f1: 0.7247658688865765\n",
            "Macro f1: 0.7110327259477911\n",
            "accuracy: 0.7247658688865765\n",
            "Loss: 0.5485245137175253\n",
            "\n",
            "\n",
            "Training Epoch 84\n",
            "Micro f1: 0.7218357927582395\n",
            "Macro f1: 0.7040610750498455\n",
            "accuracy: 0.7218357927582396\n",
            "Loss: 0.5466533285602463\n",
            "Validation Epoch 84\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7106130104425951\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5516217651446004\n",
            "\n",
            "\n",
            "Training Epoch 85\n",
            "Micro f1: 0.7202756289410387\n",
            "Macro f1: 0.7027023586878454\n",
            "accuracy: 0.7202756289410388\n",
            "Loss: 0.5427423478288115\n",
            "Validation Epoch 85\n",
            "Micro f1: 0.7190426638917794\n",
            "Macro f1: 0.7022845845902124\n",
            "accuracy: 0.7190426638917794\n",
            "Loss: 0.5525664568932589\n",
            "\n",
            "\n",
            "Training Epoch 86\n",
            "Micro f1: 0.716960280829487\n",
            "Macro f1: 0.6987820081979583\n",
            "accuracy: 0.7169602808294872\n",
            "Loss: 0.5439360179858991\n",
            "Validation Epoch 86\n",
            "Micro f1: 0.7200832466181063\n",
            "Macro f1: 0.6970465114098742\n",
            "accuracy: 0.7200832466181062\n",
            "Loss: 0.554804193579461\n",
            "\n",
            "\n",
            "Training Epoch 87\n",
            "Micro f1: 0.716960280829487\n",
            "Macro f1: 0.6984733159149241\n",
            "accuracy: 0.7169602808294872\n",
            "Loss: 0.5466755239792525\n",
            "Validation Epoch 87\n",
            "Micro f1: 0.7242455775234131\n",
            "Macro f1: 0.7101392365154107\n",
            "accuracy: 0.7242455775234131\n",
            "Loss: 0.5531226354689638\n",
            "\n",
            "\n",
            "Training Epoch 88\n",
            "Micro f1: 0.7208606903724891\n",
            "Macro f1: 0.7034931327419709\n",
            "accuracy: 0.7208606903724891\n",
            "Loss: 0.5444190048006616\n",
            "Validation Epoch 88\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.7078818007369536\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.5532349651017465\n",
            "\n",
            "\n",
            "Training Epoch 89\n",
            "Micro f1: 0.7176753559123707\n",
            "Macro f1: 0.6999387273486666\n",
            "accuracy: 0.7176753559123707\n",
            "Loss: 0.5452938669684523\n",
            "Validation Epoch 89\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.7139671737490889\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5531851351261139\n",
            "\n",
            "\n",
            "Training Epoch 90\n",
            "Micro f1: 0.7250211272183579\n",
            "Macro f1: 0.7077950368819903\n",
            "accuracy: 0.7250211272183579\n",
            "Loss: 0.5450111167997184\n",
            "Validation Epoch 90\n",
            "Micro f1: 0.7278876170655567\n",
            "Macro f1: 0.707708998304478\n",
            "accuracy: 0.7278876170655567\n",
            "Loss: 0.5472929030656815\n",
            "\n",
            "\n",
            "Training Epoch 91\n",
            "Micro f1: 0.7200806084638887\n",
            "Macro f1: 0.7021836372313951\n",
            "accuracy: 0.7200806084638887\n",
            "Loss: 0.5414914714943099\n",
            "Validation Epoch 91\n",
            "Micro f1: 0.7211238293444329\n",
            "Macro f1: 0.6979847906515864\n",
            "accuracy: 0.7211238293444329\n",
            "Loss: 0.5506259242365182\n",
            "\n",
            "\n",
            "Training Epoch 92\n",
            "Micro f1: 0.7203406357667556\n",
            "Macro f1: 0.7029208569411394\n",
            "accuracy: 0.7203406357667556\n",
            "Loss: 0.5415838308276109\n",
            "Validation Epoch 92\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.7019957526491923\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5504092352449401\n",
            "\n",
            "\n",
            "Training Epoch 93\n",
            "Micro f1: 0.7237859975297407\n",
            "Macro f1: 0.7058757687907811\n",
            "accuracy: 0.7237859975297406\n",
            "Loss: 0.5418973894723983\n",
            "Validation Epoch 93\n",
            "Micro f1: 0.7211238293444329\n",
            "Macro f1: 0.6946428402052567\n",
            "accuracy: 0.7211238293444329\n",
            "Loss: 0.5521403620065737\n",
            "\n",
            "\n",
            "Training Epoch 94\n",
            "Micro f1: 0.7273613729441593\n",
            "Macro f1: 0.7101455084660873\n",
            "accuracy: 0.7273613729441591\n",
            "Loss: 0.5404537620483714\n",
            "Validation Epoch 94\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.6905984733731325\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.5532418995603057\n",
            "\n",
            "\n",
            "Training Epoch 95\n",
            "Micro f1: 0.7213807449782227\n",
            "Macro f1: 0.7033854230306322\n",
            "accuracy: 0.7213807449782227\n",
            "Loss: 0.5447503982301561\n",
            "Validation Epoch 95\n",
            "Micro f1: 0.7232049947970863\n",
            "Macro f1: 0.706074644387643\n",
            "accuracy: 0.7232049947970863\n",
            "Loss: 0.5465969084954458\n",
            "\n",
            "\n",
            "Training Epoch 96\n",
            "Micro f1: 0.7241760384840408\n",
            "Macro f1: 0.7065707514736689\n",
            "accuracy: 0.7241760384840408\n",
            "Loss: 0.5418913800757293\n",
            "Validation Epoch 96\n",
            "Micro f1: 0.7278876170655567\n",
            "Macro f1: 0.7149809615110343\n",
            "accuracy: 0.7278876170655567\n",
            "Loss: 0.5510585049459756\n",
            "\n",
            "\n",
            "Training Epoch 97\n",
            "Micro f1: 0.7194955470324385\n",
            "Macro f1: 0.7018331796455985\n",
            "accuracy: 0.7194955470324385\n",
            "Loss: 0.5417725028094531\n",
            "Validation Epoch 97\n",
            "Micro f1: 0.714360041623309\n",
            "Macro f1: 0.6867088692157434\n",
            "accuracy: 0.714360041623309\n",
            "Loss: 0.552442375293448\n",
            "\n",
            "\n",
            "Training Epoch 98\n",
            "Micro f1: 0.7230059156211402\n",
            "Macro f1: 0.7046801755502017\n",
            "accuracy: 0.7230059156211402\n",
            "Loss: 0.5428250127640435\n",
            "Validation Epoch 98\n",
            "Micro f1: 0.7263267429760667\n",
            "Macro f1: 0.713806730682127\n",
            "accuracy: 0.7263267429760666\n",
            "Loss: 0.5520022174050986\n",
            "\n",
            "\n",
            "Training Epoch 99\n",
            "Micro f1: 0.726321263732692\n",
            "Macro f1: 0.7091180279294425\n",
            "accuracy: 0.7263212637326919\n",
            "Loss: 0.5373397856455088\n",
            "Validation Epoch 99\n",
            "Micro f1: 0.7242455775234131\n",
            "Macro f1: 0.7017363042778086\n",
            "accuracy: 0.7242455775234131\n",
            "Loss: 0.5530648862034822\n",
            "\n",
            "\n",
            "Training Epoch 100\n",
            "Micro f1: 0.7208606903724891\n",
            "Macro f1: 0.7023621793634396\n",
            "accuracy: 0.7208606903724891\n",
            "Loss: 0.5399319803547462\n",
            "Validation Epoch 100\n",
            "Micro f1: 0.7278876170655567\n",
            "Macro f1: 0.7126205209794059\n",
            "accuracy: 0.7278876170655567\n",
            "Loss: 0.5465361390970955\n",
            "\n",
            "\n",
            "Training Epoch 101\n",
            "Micro f1: 0.7280114412013261\n",
            "Macro f1: 0.7114534037740159\n",
            "accuracy: 0.7280114412013261\n",
            "Loss: 0.5389077705553812\n",
            "Validation Epoch 101\n",
            "Micro f1: 0.7122788761706554\n",
            "Macro f1: 0.6886964401772223\n",
            "accuracy: 0.7122788761706556\n",
            "Loss: 0.5519555853910683\n",
            "\n",
            "\n",
            "Training Epoch 102\n",
            "Micro f1: 0.7258012091269583\n",
            "Macro f1: 0.70840816201067\n",
            "accuracy: 0.7258012091269583\n",
            "Loss: 0.53958050441965\n",
            "Validation Epoch 102\n",
            "Micro f1: 0.7242455775234131\n",
            "Macro f1: 0.7054097761408118\n",
            "accuracy: 0.7242455775234131\n",
            "Loss: 0.5547213862257555\n",
            "\n",
            "\n",
            "Training Epoch 103\n",
            "Micro f1: 0.7210557108496392\n",
            "Macro f1: 0.7038484652061461\n",
            "accuracy: 0.7210557108496393\n",
            "Loss: 0.5408491933729941\n",
            "Validation Epoch 103\n",
            "Micro f1: 0.7273673257023934\n",
            "Macro f1: 0.7117038512844294\n",
            "accuracy: 0.7273673257023934\n",
            "Loss: 0.5520774160534883\n",
            "\n",
            "\n",
            "Training Epoch 104\n",
            "Micro f1: 0.7286615094584932\n",
            "Macro f1: 0.7113718446085566\n",
            "accuracy: 0.7286615094584932\n",
            "Loss: 0.5392176072091918\n",
            "Validation Epoch 104\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.712912821681287\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5542700827614335\n",
            "\n",
            "\n",
            "Training Epoch 105\n",
            "Micro f1: 0.722875901969707\n",
            "Macro f1: 0.7054857734215687\n",
            "accuracy: 0.7228759019697069\n",
            "Loss: 0.5381495260678061\n",
            "Validation Epoch 105\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.6986304271402242\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5525794430705141\n",
            "\n",
            "\n",
            "Training Epoch 106\n",
            "Micro f1: 0.7223558473639731\n",
            "Macro f1: 0.7046143611913742\n",
            "accuracy: 0.7223558473639732\n",
            "Loss: 0.5393572113865874\n",
            "Validation Epoch 106\n",
            "Micro f1: 0.7258064516129032\n",
            "Macro f1: 0.7078096265600864\n",
            "accuracy: 0.7258064516129032\n",
            "Loss: 0.5471219474619086\n",
            "\n",
            "\n",
            "Training Epoch 107\n",
            "Micro f1: 0.7248261067412078\n",
            "Macro f1: 0.7076368029999707\n",
            "accuracy: 0.7248261067412078\n",
            "Loss: 0.5358963627949078\n",
            "Validation Epoch 107\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.7026257110352674\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.5527383919097175\n",
            "\n",
            "\n",
            "Training Epoch 108\n",
            "Micro f1: 0.7260612364298251\n",
            "Macro f1: 0.7090172476419808\n",
            "accuracy: 0.7260612364298251\n",
            "Loss: 0.5363288570869241\n",
            "Validation Epoch 108\n",
            "Micro f1: 0.7294484911550468\n",
            "Macro f1: 0.7128574055346143\n",
            "accuracy: 0.7294484911550468\n",
            "Loss: 0.5529198333744175\n",
            "\n",
            "\n",
            "Training Epoch 109\n",
            "Micro f1: 0.7281414548527595\n",
            "Macro f1: 0.7109553699249009\n",
            "accuracy: 0.7281414548527595\n",
            "Loss: 0.5362058237287953\n",
            "Validation Epoch 109\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.7059142364397879\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5493808935496433\n",
            "\n",
            "\n",
            "Training Epoch 110\n",
            "Micro f1: 0.7268413183384255\n",
            "Macro f1: 0.7096317199264095\n",
            "accuracy: 0.7268413183384256\n",
            "Loss: 0.5337517412995102\n",
            "Validation Epoch 110\n",
            "Micro f1: 0.72840790842872\n",
            "Macro f1: 0.7111342215097156\n",
            "accuracy: 0.72840790842872\n",
            "Loss: 0.5483809659057413\n",
            "\n",
            "\n",
            "Training Epoch 111\n",
            "Micro f1: 0.7319768575700448\n",
            "Macro f1: 0.715291411044253\n",
            "accuracy: 0.7319768575700448\n",
            "Loss: 0.5340158047615366\n",
            "Validation Epoch 111\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.700763406851769\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5489041927185926\n",
            "\n",
            "\n",
            "Training Epoch 112\n",
            "Micro f1: 0.7286615094584932\n",
            "Macro f1: 0.7121240182149859\n",
            "accuracy: 0.7286615094584932\n",
            "Loss: 0.5350445272273184\n",
            "Validation Epoch 112\n",
            "Micro f1: 0.7232049947970863\n",
            "Macro f1: 0.7137113392825145\n",
            "accuracy: 0.7232049947970863\n",
            "Loss: 0.5495594970204614\n",
            "\n",
            "\n",
            "Training Epoch 113\n",
            "Micro f1: 0.7283364753299096\n",
            "Macro f1: 0.7110749653451389\n",
            "accuracy: 0.7283364753299096\n",
            "Loss: 0.5365533091577076\n",
            "Validation Epoch 113\n",
            "Micro f1: 0.7242455775234131\n",
            "Macro f1: 0.711099086099086\n",
            "accuracy: 0.7242455775234131\n",
            "Loss: 0.5511366582113849\n",
            "\n",
            "\n",
            "Training Epoch 114\n",
            "Micro f1: 0.7272313592927259\n",
            "Macro f1: 0.7105108659109738\n",
            "accuracy: 0.7272313592927258\n",
            "Loss: 0.5344585068136639\n",
            "Validation Epoch 114\n",
            "Micro f1: 0.7252861602497399\n",
            "Macro f1: 0.7097995166070528\n",
            "accuracy: 0.7252861602497399\n",
            "Loss: 0.5491111832709352\n",
            "\n",
            "\n",
            "Training Epoch 115\n",
            "Micro f1: 0.7285314958070598\n",
            "Macro f1: 0.711505952804382\n",
            "accuracy: 0.7285314958070598\n",
            "Loss: 0.5320870644740157\n",
            "Validation Epoch 115\n",
            "Micro f1: 0.7190426638917794\n",
            "Macro f1: 0.7106148055207027\n",
            "accuracy: 0.7190426638917794\n",
            "Loss: 0.556367994093698\n",
            "\n",
            "\n",
            "Training Epoch 116\n",
            "Micro f1: 0.7299616459728273\n",
            "Macro f1: 0.7129099766733731\n",
            "accuracy: 0.7299616459728272\n",
            "Loss: 0.5327636665527141\n",
            "Validation Epoch 116\n",
            "Micro f1: 0.7232049947970863\n",
            "Macro f1: 0.7150059311981021\n",
            "accuracy: 0.7232049947970863\n",
            "Loss: 0.557959109544754\n",
            "\n",
            "\n",
            "Training Epoch 117\n",
            "Micro f1: 0.7332119872586622\n",
            "Macro f1: 0.7173051614271888\n",
            "accuracy: 0.7332119872586622\n",
            "Loss: 0.5311094092789905\n",
            "Validation Epoch 117\n",
            "Micro f1: 0.7200832466181063\n",
            "Macro f1: 0.7124073282959087\n",
            "accuracy: 0.7200832466181062\n",
            "Loss: 0.5582288646500958\n",
            "\n",
            "\n",
            "Training Epoch 118\n",
            "Micro f1: 0.7316518234414613\n",
            "Macro f1: 0.715577535807091\n",
            "accuracy: 0.7316518234414614\n",
            "Loss: 0.5327498130732663\n",
            "Validation Epoch 118\n",
            "Micro f1: 0.7273673257023934\n",
            "Macro f1: 0.7047463251892374\n",
            "accuracy: 0.7273673257023934\n",
            "Loss: 0.5474015883177765\n",
            "\n",
            "\n",
            "Training Epoch 119\n",
            "Micro f1: 0.7297016186699603\n",
            "Macro f1: 0.7126141626369531\n",
            "accuracy: 0.7297016186699603\n",
            "Loss: 0.5317934378207102\n",
            "Validation Epoch 119\n",
            "Micro f1: 0.7252861602497399\n",
            "Macro f1: 0.7100926614719565\n",
            "accuracy: 0.7252861602497399\n",
            "Loss: 0.5471507004223579\n",
            "\n",
            "\n",
            "Training Epoch 120\n",
            "Micro f1: 0.7258012091269583\n",
            "Macro f1: 0.708388462782193\n",
            "accuracy: 0.7258012091269583\n",
            "Loss: 0.534130263136479\n",
            "Validation Epoch 120\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.716779016301728\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5554421972637335\n",
            "\n",
            "\n",
            "Training Epoch 121\n",
            "Micro f1: 0.7308067347071444\n",
            "Macro f1: 0.7146350514160631\n",
            "accuracy: 0.7308067347071443\n",
            "Loss: 0.5334156819322996\n",
            "Validation Epoch 121\n",
            "Micro f1: 0.7242455775234131\n",
            "Macro f1: 0.715229011047994\n",
            "accuracy: 0.7242455775234131\n",
            "Loss: 0.5497900716159955\n",
            "\n",
            "\n",
            "Training Epoch 122\n",
            "Micro f1: 0.7304166937528441\n",
            "Macro f1: 0.7139767196988441\n",
            "accuracy: 0.730416693752844\n",
            "Loss: 0.5322670074733528\n",
            "Validation Epoch 122\n",
            "Micro f1: 0.7232049947970863\n",
            "Macro f1: 0.7110630352024161\n",
            "accuracy: 0.7232049947970863\n",
            "Loss: 0.5463586604792224\n",
            "\n",
            "\n",
            "Training Epoch 123\n",
            "Micro f1: 0.7325619190014953\n",
            "Macro f1: 0.7166348491475268\n",
            "accuracy: 0.7325619190014951\n",
            "Loss: 0.527993998569659\n",
            "Validation Epoch 123\n",
            "Micro f1: 0.7154006243496358\n",
            "Macro f1: 0.6867788482307435\n",
            "accuracy: 0.7154006243496358\n",
            "Loss: 0.5566038794753966\n",
            "\n",
            "\n",
            "Training Epoch 124\n",
            "Micro f1: 0.7305467074042774\n",
            "Macro f1: 0.7138486834822978\n",
            "accuracy: 0.7305467074042774\n",
            "Loss: 0.533546104872301\n",
            "Validation Epoch 124\n",
            "Micro f1: 0.7216441207075962\n",
            "Macro f1: 0.705118267284838\n",
            "accuracy: 0.7216441207075962\n",
            "Loss: 0.5494073321504042\n",
            "\n",
            "\n",
            "Training Epoch 125\n",
            "Micro f1: 0.7325619190014953\n",
            "Macro f1: 0.7160550083971642\n",
            "accuracy: 0.7325619190014951\n",
            "Loss: 0.5310001388316095\n",
            "Validation Epoch 125\n",
            "Micro f1: 0.7211238293444329\n",
            "Macro f1: 0.6974152920719958\n",
            "accuracy: 0.7211238293444329\n",
            "Loss: 0.5545340377437181\n",
            "\n",
            "\n",
            "Training Epoch 126\n",
            "Micro f1: 0.7305467074042774\n",
            "Macro f1: 0.7139629812024313\n",
            "accuracy: 0.7305467074042774\n",
            "Loss: 0.5290467658558408\n",
            "Validation Epoch 126\n",
            "Micro f1: 0.7252861602497399\n",
            "Macro f1: 0.7085942103395164\n",
            "accuracy: 0.7252861602497399\n",
            "Loss: 0.5522143968865891\n",
            "\n",
            "\n",
            "Training Epoch 127\n",
            "Micro f1: 0.7321068712214783\n",
            "Macro f1: 0.7158638083657807\n",
            "accuracy: 0.7321068712214782\n",
            "Loss: 0.5320303779165115\n",
            "Validation Epoch 127\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.7019957526491923\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5505358596478612\n",
            "\n",
            "\n",
            "Training Epoch 128\n",
            "Micro f1: 0.7296366118442437\n",
            "Macro f1: 0.7132437706850963\n",
            "accuracy: 0.7296366118442437\n",
            "Loss: 0.5296763003554017\n",
            "Validation Epoch 128\n",
            "Micro f1: 0.72840790842872\n",
            "Macro f1: 0.7078367284180707\n",
            "accuracy: 0.72840790842872\n",
            "Loss: 0.558816743291114\n",
            "\n",
            "\n",
            "Training Epoch 129\n",
            "Micro f1: 0.7341870896444127\n",
            "Macro f1: 0.7174687977415861\n",
            "accuracy: 0.7341870896444127\n",
            "Loss: 0.5294897745589953\n",
            "Validation Epoch 129\n",
            "Micro f1: 0.7091571279916753\n",
            "Macro f1: 0.6881358155957346\n",
            "accuracy: 0.7091571279916753\n",
            "Loss: 0.5610483628659209\n",
            "\n",
            "\n",
            "Training Epoch 130\n",
            "Micro f1: 0.7339270623415459\n",
            "Macro f1: 0.7173251994977703\n",
            "accuracy: 0.7339270623415458\n",
            "Loss: 0.5266294383643323\n",
            "Validation Epoch 130\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7026819079843106\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5463135215369138\n",
            "\n",
            "\n",
            "Training Epoch 131\n",
            "Micro f1: 0.7347071442501463\n",
            "Macro f1: 0.7187882245100496\n",
            "accuracy: 0.7347071442501463\n",
            "Loss: 0.5264374131255487\n",
            "Validation Epoch 131\n",
            "Micro f1: 0.7263267429760667\n",
            "Macro f1: 0.7086038886173087\n",
            "accuracy: 0.7263267429760666\n",
            "Loss: 0.552758826450868\n",
            "\n",
            "\n",
            "Training Epoch 132\n",
            "Micro f1: 0.7302866801014106\n",
            "Macro f1: 0.7134000637547678\n",
            "accuracy: 0.7302866801014106\n",
            "Loss: 0.5298107219646974\n",
            "Validation Epoch 132\n",
            "Micro f1: 0.7190426638917794\n",
            "Macro f1: 0.7060552815908157\n",
            "accuracy: 0.7190426638917794\n",
            "Loss: 0.5489039419850043\n",
            "\n",
            "\n",
            "Training Epoch 133\n",
            "Micro f1: 0.7350971852044464\n",
            "Macro f1: 0.7191832490993618\n",
            "accuracy: 0.7350971852044464\n",
            "Loss: 0.5235166393545709\n",
            "Validation Epoch 133\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7036831304472349\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.554927503274492\n",
            "\n",
            "\n",
            "Training Epoch 134\n",
            "Micro f1: 0.7336670350386789\n",
            "Macro f1: 0.7179069692589382\n",
            "accuracy: 0.733667035038679\n",
            "Loss: 0.5265861261101622\n",
            "Validation Epoch 134\n",
            "Micro f1: 0.7159209157127991\n",
            "Macro f1: 0.6887366304212419\n",
            "accuracy: 0.7159209157127991\n",
            "Loss: 0.5537980327552016\n",
            "\n",
            "\n",
            "Training Epoch 135\n",
            "Micro f1: 0.7333420009100956\n",
            "Macro f1: 0.7172390243633138\n",
            "accuracy: 0.7333420009100956\n",
            "Loss: 0.5262489154191374\n",
            "Validation Epoch 135\n",
            "Micro f1: 0.7211238293444329\n",
            "Macro f1: 0.704949959674463\n",
            "accuracy: 0.7211238293444329\n",
            "Loss: 0.5500366246897327\n",
            "\n",
            "\n",
            "Training Epoch 136\n",
            "Micro f1: 0.7341220828186961\n",
            "Macro f1: 0.7180105035775951\n",
            "accuracy: 0.734122082818696\n",
            "Loss: 0.5275480352388598\n",
            "Validation Epoch 136\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.7116702710872731\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5490397865122015\n",
            "\n",
            "\n",
            "Training Epoch 137\n",
            "Micro f1: 0.734967171553013\n",
            "Macro f1: 0.7191190694234493\n",
            "accuracy: 0.734967171553013\n",
            "Loss: 0.5264029545620424\n",
            "Validation Epoch 137\n",
            "Micro f1: 0.7232049947970863\n",
            "Macro f1: 0.7057593250188485\n",
            "accuracy: 0.7232049947970863\n",
            "Loss: 0.5524256071768516\n",
            "\n",
            "\n",
            "Training Epoch 138\n",
            "Micro f1: 0.7323018916986284\n",
            "Macro f1: 0.7162293385649432\n",
            "accuracy: 0.7323018916986284\n",
            "Loss: 0.526827756431643\n",
            "Validation Epoch 138\n",
            "Micro f1: 0.7278876170655567\n",
            "Macro f1: 0.7203767936378033\n",
            "accuracy: 0.7278876170655567\n",
            "Loss: 0.5542293218049136\n",
            "\n",
            "\n",
            "Training Epoch 139\n",
            "Micro f1: 0.7377624650588311\n",
            "Macro f1: 0.7218714844577063\n",
            "accuracy: 0.7377624650588311\n",
            "Loss: 0.5233434365903513\n",
            "Validation Epoch 139\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.6974577104926589\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.5509752090558533\n",
            "\n",
            "\n",
            "Training Epoch 140\n",
            "Micro f1: 0.7356822466358968\n",
            "Macro f1: 0.71925562220581\n",
            "accuracy: 0.7356822466358968\n",
            "Loss: 0.5266834494973419\n",
            "Validation Epoch 140\n",
            "Micro f1: 0.7185223725286161\n",
            "Macro f1: 0.705034348766284\n",
            "accuracy: 0.7185223725286161\n",
            "Loss: 0.5541105327034784\n",
            "\n",
            "\n",
            "Training Epoch 141\n",
            "Micro f1: 0.7380874991874147\n",
            "Macro f1: 0.7226611488958429\n",
            "accuracy: 0.7380874991874147\n",
            "Loss: 0.5206424031027141\n",
            "Validation Epoch 141\n",
            "Micro f1: 0.7200832466181063\n",
            "Macro f1: 0.7032316205170592\n",
            "accuracy: 0.7200832466181062\n",
            "Loss: 0.5563252619967973\n",
            "\n",
            "\n",
            "Training Epoch 142\n",
            "Micro f1: 0.73522719885588\n",
            "Macro f1: 0.7191733296466452\n",
            "accuracy: 0.7352271988558798\n",
            "Loss: 0.5227759664804673\n",
            "Validation Epoch 142\n",
            "Micro f1: 0.7252861602497399\n",
            "Macro f1: 0.7096517248956273\n",
            "accuracy: 0.7252861602497399\n",
            "Loss: 0.5473684546314488\n",
            "\n",
            "\n",
            "Training Epoch 143\n",
            "Micro f1: 0.739517649353182\n",
            "Macro f1: 0.7239777674852537\n",
            "accuracy: 0.7395176493531821\n",
            "Loss: 0.5207750091712589\n",
            "Validation Epoch 143\n",
            "Micro f1: 0.7086368366285121\n",
            "Macro f1: 0.6887348578091597\n",
            "accuracy: 0.708636836628512\n",
            "Loss: 0.5591592466043047\n",
            "\n",
            "\n",
            "Training Epoch 144\n",
            "Micro f1: 0.7363973217187805\n",
            "Macro f1: 0.7200244881593323\n",
            "accuracy: 0.7363973217187805\n",
            "Loss: 0.523274960890877\n",
            "Validation Epoch 144\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7145220274217986\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5531840294845833\n",
            "\n",
            "\n",
            "Training Epoch 145\n",
            "Micro f1: 0.7404927517389326\n",
            "Macro f1: 0.7257641608445135\n",
            "accuracy: 0.7404927517389326\n",
            "Loss: 0.5191112079671167\n",
            "Validation Epoch 145\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.6960618220337504\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.5480564524812147\n",
            "\n",
            "\n",
            "Training Epoch 146\n",
            "Micro f1: 0.7382825196645648\n",
            "Macro f1: 0.7218871911316531\n",
            "accuracy: 0.7382825196645648\n",
            "Loss: 0.5222462204879386\n",
            "Validation Epoch 146\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7044829809421745\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5474348584236193\n",
            "\n",
            "\n",
            "Training Epoch 147\n",
            "Micro f1: 0.7392576220503153\n",
            "Macro f1: 0.7241500074243321\n",
            "accuracy: 0.7392576220503153\n",
            "Loss: 0.521115415867054\n",
            "Validation Epoch 147\n",
            "Micro f1: 0.7258064516129032\n",
            "Macro f1: 0.7108590497314533\n",
            "accuracy: 0.7258064516129032\n",
            "Loss: 0.5483817729082975\n",
            "\n",
            "\n",
            "Training Epoch 148\n",
            "Micro f1: 0.7354872261587466\n",
            "Macro f1: 0.7196885654758381\n",
            "accuracy: 0.7354872261587466\n",
            "Loss: 0.5256154845137606\n",
            "Validation Epoch 148\n",
            "Micro f1: 0.7252861602497399\n",
            "Macro f1: 0.7124554579332482\n",
            "accuracy: 0.7252861602497399\n",
            "Loss: 0.5533582252904403\n",
            "\n",
            "\n",
            "Training Epoch 149\n",
            "Micro f1: 0.7375674445816811\n",
            "Macro f1: 0.7218201573994293\n",
            "accuracy: 0.7375674445816811\n",
            "Loss: 0.5198227607193955\n",
            "Validation Epoch 149\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7113417225451213\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5601048479395464\n",
            "\n",
            "\n",
            "Training Epoch 150\n",
            "Micro f1: 0.7367873626730806\n",
            "Macro f1: 0.721410828859647\n",
            "accuracy: 0.7367873626730806\n",
            "Loss: 0.5200795238361289\n",
            "Validation Epoch 150\n",
            "Micro f1: 0.7268470343392299\n",
            "Macro f1: 0.7129429569962065\n",
            "accuracy: 0.7268470343392299\n",
            "Loss: 0.5538577109821572\n",
            "\n",
            "\n",
            "Training Epoch 151\n",
            "Micro f1: 0.7369823831502309\n",
            "Macro f1: 0.7215541678353649\n",
            "accuracy: 0.7369823831502308\n",
            "Loss: 0.520027080639856\n",
            "Validation Epoch 151\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.6994447491482665\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.5503225432447165\n",
            "\n",
            "\n",
            "Training Epoch 152\n",
            "Micro f1: 0.743418058896184\n",
            "Macro f1: 0.7278787620390667\n",
            "accuracy: 0.7434180588961841\n",
            "Loss: 0.5161857739896387\n",
            "Validation Epoch 152\n",
            "Micro f1: 0.7242455775234131\n",
            "Macro f1: 0.7037097795027877\n",
            "accuracy: 0.7242455775234131\n",
            "Loss: 0.5557541339850622\n",
            "\n",
            "\n",
            "Training Epoch 153\n",
            "Micro f1: 0.7406877722160827\n",
            "Macro f1: 0.725751177809662\n",
            "accuracy: 0.7406877722160827\n",
            "Loss: 0.5180347110309372\n",
            "Validation Epoch 153\n",
            "Micro f1: 0.7174817898022893\n",
            "Macro f1: 0.7101835876612963\n",
            "accuracy: 0.7174817898022893\n",
            "Loss: 0.5560430906528284\n",
            "\n",
            "\n",
            "Training Epoch 154\n",
            "Micro f1: 0.7378924787102645\n",
            "Macro f1: 0.7222283905202633\n",
            "accuracy: 0.7378924787102645\n",
            "Loss: 0.5189891523389212\n",
            "Validation Epoch 154\n",
            "Micro f1: 0.7252861602497399\n",
            "Macro f1: 0.7106692592360284\n",
            "accuracy: 0.7252861602497399\n",
            "Loss: 0.5561118153008547\n",
            "\n",
            "\n",
            "Training Epoch 155\n",
            "Micro f1: 0.7400377039589157\n",
            "Macro f1: 0.7243662623763923\n",
            "accuracy: 0.7400377039589157\n",
            "Loss: 0.516278646932818\n",
            "Validation Epoch 155\n",
            "Micro f1: 0.7216441207075962\n",
            "Macro f1: 0.7010025126059192\n",
            "accuracy: 0.7216441207075962\n",
            "Loss: 0.5566269143307505\n",
            "\n",
            "\n",
            "Training Epoch 156\n",
            "Micro f1: 0.7368523694987974\n",
            "Macro f1: 0.7216501610297753\n",
            "accuracy: 0.7368523694987974\n",
            "Loss: 0.5222027679428984\n",
            "Validation Epoch 156\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7085089922602149\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5490544283193005\n",
            "\n",
            "\n",
            "Training Epoch 157\n",
            "Micro f1: 0.7404277449132158\n",
            "Macro f1: 0.7251749060202968\n",
            "accuracy: 0.7404277449132158\n",
            "Loss: 0.5198131316881666\n",
            "Validation Epoch 157\n",
            "Micro f1: 0.7185223725286161\n",
            "Macro f1: 0.7021207597481639\n",
            "accuracy: 0.7185223725286161\n",
            "Loss: 0.5546725537658723\n",
            "\n",
            "\n",
            "Training Epoch 158\n",
            "Micro f1: 0.7427029838133004\n",
            "Macro f1: 0.7283410801551553\n",
            "accuracy: 0.7427029838133004\n",
            "Loss: 0.5187278630177098\n",
            "Validation Epoch 158\n",
            "Micro f1: 0.7070759625390218\n",
            "Macro f1: 0.6814638780191924\n",
            "accuracy: 0.7070759625390218\n",
            "Loss: 0.5631380439543527\n",
            "\n",
            "\n",
            "Training Epoch 159\n",
            "Micro f1: 0.7407527790417993\n",
            "Macro f1: 0.7257758997496896\n",
            "accuracy: 0.7407527790417994\n",
            "Loss: 0.5177049350280029\n",
            "Validation Epoch 159\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7040099114407234\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5564017970699909\n",
            "\n",
            "\n",
            "Training Epoch 160\n",
            "Micro f1: 0.7407527790417993\n",
            "Macro f1: 0.7260563864562619\n",
            "accuracy: 0.7407527790417994\n",
            "Loss: 0.5170626514852666\n",
            "Validation Epoch 160\n",
            "Micro f1: 0.7211238293444329\n",
            "Macro f1: 0.7022418455483914\n",
            "accuracy: 0.7211238293444329\n",
            "Loss: 0.549731835353473\n",
            "\n",
            "\n",
            "Training Epoch 161\n",
            "Micro f1: 0.7399726971331991\n",
            "Macro f1: 0.7248444547473498\n",
            "accuracy: 0.739972697133199\n",
            "Loss: 0.5167888032423483\n",
            "Validation Epoch 161\n",
            "Micro f1: 0.7216441207075962\n",
            "Macro f1: 0.7092387193462417\n",
            "accuracy: 0.7216441207075962\n",
            "Loss: 0.5523780633595364\n",
            "\n",
            "\n",
            "Training Epoch 162\n",
            "Micro f1: 0.7431580315933173\n",
            "Macro f1: 0.7286238849831188\n",
            "accuracy: 0.7431580315933173\n",
            "Loss: 0.5121319736053939\n",
            "Validation Epoch 162\n",
            "Micro f1: 0.7211238293444329\n",
            "Macro f1: 0.7054025395859476\n",
            "accuracy: 0.7211238293444329\n",
            "Loss: 0.5515529666680935\n",
            "\n",
            "\n",
            "Training Epoch 163\n",
            "Micro f1: 0.7441981408047845\n",
            "Macro f1: 0.7287775049890233\n",
            "accuracy: 0.7441981408047845\n",
            "Loss: 0.5143454446149467\n",
            "Validation Epoch 163\n",
            "Micro f1: 0.7206035379812695\n",
            "Macro f1: 0.7096688987624317\n",
            "accuracy: 0.7206035379812695\n",
            "Loss: 0.5583293307418665\n",
            "\n",
            "\n",
            "Training Epoch 164\n",
            "Micro f1: 0.7444581681076513\n",
            "Macro f1: 0.7299288297527804\n",
            "accuracy: 0.7444581681076513\n",
            "Loss: 0.513468591484731\n",
            "Validation Epoch 164\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7031539813418293\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5495917763838098\n",
            "\n",
            "\n",
            "Training Epoch 165\n",
            "Micro f1: 0.7454982773191186\n",
            "Macro f1: 0.7307868949634504\n",
            "accuracy: 0.7454982773191186\n",
            "Loss: 0.5132418065922424\n",
            "Validation Epoch 165\n",
            "Micro f1: 0.7159209157127991\n",
            "Macro f1: 0.6986599220556362\n",
            "accuracy: 0.7159209157127991\n",
            "Loss: 0.5515591782971847\n",
            "\n",
            "\n",
            "Training Epoch 166\n",
            "Micro f1: 0.7417278814275498\n",
            "Macro f1: 0.7264269965424829\n",
            "accuracy: 0.7417278814275499\n",
            "Loss: 0.5152851846262719\n",
            "Validation Epoch 166\n",
            "Micro f1: 0.7206035379812695\n",
            "Macro f1: 0.7085414193990008\n",
            "accuracy: 0.7206035379812695\n",
            "Loss: 0.5497259401585445\n",
            "\n",
            "\n",
            "Training Epoch 167\n",
            "Micro f1: 0.7345771305987129\n",
            "Macro f1: 0.7193065679395667\n",
            "accuracy: 0.7345771305987129\n",
            "Loss: 0.5204127594332933\n",
            "Validation Epoch 167\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.7047708722950954\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5523501506521682\n",
            "\n",
            "\n",
            "Training Epoch 168\n",
            "Micro f1: 0.741272833647533\n",
            "Macro f1: 0.7265536706812639\n",
            "accuracy: 0.7412728336475329\n",
            "Loss: 0.5131495368208062\n",
            "Validation Epoch 168\n",
            "Micro f1: 0.7133194588969823\n",
            "Macro f1: 0.6967710365376152\n",
            "accuracy: 0.7133194588969823\n",
            "Loss: 0.5526587384298813\n",
            "\n",
            "\n",
            "Training Epoch 169\n",
            "Micro f1: 0.7435480725476175\n",
            "Macro f1: 0.7284785885925549\n",
            "accuracy: 0.7435480725476175\n",
            "Loss: 0.5121095225741611\n",
            "Validation Epoch 169\n",
            "Micro f1: 0.7133194588969823\n",
            "Macro f1: 0.6956584088064683\n",
            "accuracy: 0.7133194588969823\n",
            "Loss: 0.5502307584955672\n",
            "\n",
            "\n",
            "Training Epoch 170\n",
            "Micro f1: 0.7401027107846324\n",
            "Macro f1: 0.7252292578522362\n",
            "accuracy: 0.7401027107846324\n",
            "Loss: 0.5141482807835274\n",
            "Validation Epoch 170\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.7035763513044389\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.5576940654230512\n",
            "\n",
            "\n",
            "Training Epoch 171\n",
            "Micro f1: 0.7434830657219008\n",
            "Macro f1: 0.7285240048085002\n",
            "accuracy: 0.7434830657219008\n",
            "Loss: 0.5135938167850838\n",
            "Validation Epoch 171\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.7065422786264087\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.555329252988839\n",
            "\n",
            "\n",
            "Training Epoch 172\n",
            "Micro f1: 0.7479035298706365\n",
            "Macro f1: 0.7337140140838683\n",
            "accuracy: 0.7479035298706365\n",
            "Loss: 0.5100173336955217\n",
            "Validation Epoch 172\n",
            "Micro f1: 0.7117585848074922\n",
            "Macro f1: 0.7007896230603654\n",
            "accuracy: 0.7117585848074922\n",
            "Loss: 0.5563717164283941\n",
            "\n",
            "\n",
            "Training Epoch 173\n",
            "Micro f1: 0.7440681271533511\n",
            "Macro f1: 0.7287819813906058\n",
            "accuracy: 0.7440681271533511\n",
            "Loss: 0.5135027875585516\n",
            "Validation Epoch 173\n",
            "Micro f1: 0.7185223725286161\n",
            "Macro f1: 0.7005388159757252\n",
            "accuracy: 0.7185223725286161\n",
            "Loss: 0.5519176168628961\n",
            "\n",
            "\n",
            "Training Epoch 174\n",
            "Micro f1: 0.743873106676201\n",
            "Macro f1: 0.7287963266409989\n",
            "accuracy: 0.743873106676201\n",
            "Loss: 0.5103633936161559\n",
            "Validation Epoch 174\n",
            "Micro f1: 0.7206035379812695\n",
            "Macro f1: 0.7040159056672113\n",
            "accuracy: 0.7206035379812695\n",
            "Loss: 0.5511701558985986\n",
            "\n",
            "\n",
            "Training Epoch 175\n",
            "Micro f1: 0.7437430930247676\n",
            "Macro f1: 0.7289040952292459\n",
            "accuracy: 0.7437430930247676\n",
            "Loss: 0.5101437005146625\n",
            "Validation Epoch 175\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7065017838412238\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5532533155738815\n",
            "\n",
            "\n",
            "Training Epoch 176\n",
            "Micro f1: 0.7454982773191186\n",
            "Macro f1: 0.7314192892140539\n",
            "accuracy: 0.7454982773191186\n",
            "Loss: 0.5101626698322702\n",
            "Validation Epoch 176\n",
            "Micro f1: 0.7273673257023934\n",
            "Macro f1: 0.7092321354950393\n",
            "accuracy: 0.7273673257023934\n",
            "Loss: 0.5506881709433784\n",
            "\n",
            "\n",
            "Training Epoch 177\n",
            "Micro f1: 0.7456932977962686\n",
            "Macro f1: 0.7308457372325221\n",
            "accuracy: 0.7456932977962686\n",
            "Loss: 0.5138781388559361\n",
            "Validation Epoch 177\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7095312667024498\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5457915905093359\n",
            "\n",
            "\n",
            "Training Epoch 178\n",
            "Micro f1: 0.7466033933563024\n",
            "Macro f1: 0.7321874895265242\n",
            "accuracy: 0.7466033933563024\n",
            "Loss: 0.509174021607625\n",
            "Validation Epoch 178\n",
            "Micro f1: 0.7237252861602498\n",
            "Macro f1: 0.7042678500797587\n",
            "accuracy: 0.7237252861602498\n",
            "Loss: 0.5507820204269788\n",
            "\n",
            "\n",
            "Training Epoch 179\n",
            "Micro f1: 0.7443931612819346\n",
            "Macro f1: 0.7299203207659105\n",
            "accuracy: 0.7443931612819346\n",
            "Loss: 0.5082543026539739\n",
            "Validation Epoch 179\n",
            "Micro f1: 0.7174817898022893\n",
            "Macro f1: 0.6999171041724233\n",
            "accuracy: 0.7174817898022893\n",
            "Loss: 0.5566126196837622\n",
            "\n",
            "\n",
            "Training Epoch 180\n",
            "Micro f1: 0.7510238575050382\n",
            "Macro f1: 0.7370765781236166\n",
            "accuracy: 0.7510238575050381\n",
            "Loss: 0.5031307895751108\n",
            "Validation Epoch 180\n",
            "Micro f1: 0.7190426638917794\n",
            "Macro f1: 0.7021283923405428\n",
            "accuracy: 0.7190426638917794\n",
            "Loss: 0.5518523593825743\n",
            "\n",
            "\n",
            "Training Epoch 181\n",
            "Micro f1: 0.7469934343106025\n",
            "Macro f1: 0.7323942783134305\n",
            "accuracy: 0.7469934343106026\n",
            "Loss: 0.5110844132527245\n",
            "Validation Epoch 181\n",
            "Micro f1: 0.7206035379812695\n",
            "Macro f1: 0.7009262438659707\n",
            "accuracy: 0.7206035379812695\n",
            "Loss: 0.5516219646477503\n",
            "\n",
            "\n",
            "Training Epoch 182\n",
            "Micro f1: 0.7435480725476175\n",
            "Macro f1: 0.7287239592926722\n",
            "accuracy: 0.7435480725476175\n",
            "Loss: 0.5091049591710562\n",
            "Validation Epoch 182\n",
            "Micro f1: 0.7200832466181063\n",
            "Macro f1: 0.6949188347131334\n",
            "accuracy: 0.7200832466181062\n",
            "Loss: 0.5666528804735704\n",
            "\n",
            "\n",
            "Training Epoch 183\n",
            "Micro f1: 0.7497237209907041\n",
            "Macro f1: 0.7355527047173845\n",
            "accuracy: 0.7497237209907041\n",
            "Loss: 0.5098080313812918\n",
            "Validation Epoch 183\n",
            "Micro f1: 0.714360041623309\n",
            "Macro f1: 0.7058454401886731\n",
            "accuracy: 0.714360041623309\n",
            "Loss: 0.5568757414325209\n",
            "\n",
            "\n",
            "Training Epoch 184\n",
            "Micro f1: 0.74842358447637\n",
            "Macro f1: 0.7337354302376944\n",
            "accuracy: 0.74842358447637\n",
            "Loss: 0.5039609231349088\n",
            "Validation Epoch 184\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.7111968200743148\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.559574054415561\n",
            "\n",
            "\n",
            "Training Epoch 185\n",
            "Micro f1: 0.7481635571735032\n",
            "Macro f1: 0.7340728615490415\n",
            "accuracy: 0.7481635571735032\n",
            "Loss: 0.5072617981420238\n",
            "Validation Epoch 185\n",
            "Micro f1: 0.7195629552549427\n",
            "Macro f1: 0.6991190743876927\n",
            "accuracy: 0.7195629552549427\n",
            "Loss: 0.5519813263465551\n",
            "\n",
            "\n",
            "Training Epoch 186\n",
            "Micro f1: 0.7518689462393552\n",
            "Macro f1: 0.7379441367032771\n",
            "accuracy: 0.7518689462393552\n",
            "Loss: 0.5066856930570642\n",
            "Validation Epoch 186\n",
            "Micro f1: 0.7242455775234131\n",
            "Macro f1: 0.7022854804769698\n",
            "accuracy: 0.7242455775234131\n",
            "Loss: 0.552537016385843\n",
            "\n",
            "\n",
            "Training Epoch 187\n",
            "Micro f1: 0.7482935708249366\n",
            "Macro f1: 0.7337523279415242\n",
            "accuracy: 0.7482935708249366\n",
            "Loss: 0.5054238734151123\n",
            "Validation Epoch 187\n",
            "Micro f1: 0.7200832466181063\n",
            "Macro f1: 0.7007945739798723\n",
            "accuracy: 0.7200832466181062\n",
            "Loss: 0.5568988212376587\n",
            "\n",
            "\n",
            "Training Epoch 188\n",
            "Micro f1: 0.7437430930247676\n",
            "Macro f1: 0.7289389661016241\n",
            "accuracy: 0.7437430930247676\n",
            "Loss: 0.5090761913176386\n",
            "Validation Epoch 188\n",
            "Micro f1: 0.7252861602497399\n",
            "Macro f1: 0.7055158324821247\n",
            "accuracy: 0.7252861602497399\n",
            "Loss: 0.5651280445254538\n",
            "\n",
            "\n",
            "Training Epoch 189\n",
            "Micro f1: 0.7514789052850549\n",
            "Macro f1: 0.737315238527426\n",
            "accuracy: 0.7514789052850549\n",
            "Loss: 0.5019273797898184\n",
            "Validation Epoch 189\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.6950541236790768\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.5633162361038618\n",
            "\n",
            "\n",
            "Training Epoch 190\n",
            "Micro f1: 0.7490086459078203\n",
            "Macro f1: 0.7347549362567266\n",
            "accuracy: 0.7490086459078203\n",
            "Loss: 0.5074013302609975\n",
            "Validation Epoch 190\n",
            "Micro f1: 0.7216441207075962\n",
            "Macro f1: 0.7035369309540633\n",
            "accuracy: 0.7216441207075962\n",
            "Loss: 0.5527263084226404\n",
            "\n",
            "\n",
            "Training Epoch 191\n",
            "Micro f1: 0.7497887278164207\n",
            "Macro f1: 0.7357471789811143\n",
            "accuracy: 0.7497887278164207\n",
            "Loss: 0.5050300804747118\n",
            "Validation Epoch 191\n",
            "Micro f1: 0.7221644120707597\n",
            "Macro f1: 0.7086463518767487\n",
            "accuracy: 0.7221644120707597\n",
            "Loss: 0.5495173743933686\n",
            "\n",
            "\n",
            "Training Epoch 192\n",
            "Micro f1: 0.7457583046219853\n",
            "Macro f1: 0.7313028881106904\n",
            "accuracy: 0.7457583046219853\n",
            "Loss: 0.5066964822722572\n",
            "Validation Epoch 192\n",
            "Micro f1: 0.714360041623309\n",
            "Macro f1: 0.705520352646172\n",
            "accuracy: 0.714360041623309\n",
            "Loss: 0.5574631388029776\n",
            "\n",
            "\n",
            "Training Epoch 193\n",
            "Micro f1: 0.7503087824221543\n",
            "Macro f1: 0.7360109610080969\n",
            "accuracy: 0.7503087824221544\n",
            "Loss: 0.5043500947481382\n",
            "Validation Epoch 193\n",
            "Micro f1: 0.7117585848074922\n",
            "Macro f1: 0.7038543863864932\n",
            "accuracy: 0.7117585848074922\n",
            "Loss: 0.5593714024409775\n",
            "\n",
            "\n",
            "Training Epoch 194\n",
            "Micro f1: 0.7480985503477865\n",
            "Macro f1: 0.7336400641931059\n",
            "accuracy: 0.7480985503477865\n",
            "Loss: 0.5077232255788207\n",
            "Validation Epoch 194\n",
            "Micro f1: 0.7127991675338188\n",
            "Macro f1: 0.7048200614986028\n",
            "accuracy: 0.7127991675338189\n",
            "Loss: 0.556835092296285\n",
            "\n",
            "\n",
            "Training Epoch 195\n",
            "Micro f1: 0.7515439121107717\n",
            "Macro f1: 0.737625765427797\n",
            "accuracy: 0.7515439121107717\n",
            "Loss: 0.49870669319761024\n",
            "Validation Epoch 195\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.7062776248312482\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.5571985881436955\n",
            "\n",
            "\n",
            "Training Epoch 196\n",
            "Micro f1: 0.7546642397451733\n",
            "Macro f1: 0.7405576669485632\n",
            "accuracy: 0.7546642397451733\n",
            "Loss: 0.5019512532468645\n",
            "Validation Epoch 196\n",
            "Micro f1: 0.7148803329864724\n",
            "Macro f1: 0.7004391353811148\n",
            "accuracy: 0.7148803329864725\n",
            "Loss: 0.5557570735777705\n",
            "\n",
            "\n",
            "Training Epoch 197\n",
            "Micro f1: 0.7508938438536045\n",
            "Macro f1: 0.7369226384880017\n",
            "accuracy: 0.7508938438536046\n",
            "Loss: 0.5008883057413874\n",
            "Validation Epoch 197\n",
            "Micro f1: 0.7278876170655567\n",
            "Macro f1: 0.707189969714209\n",
            "accuracy: 0.7278876170655567\n",
            "Loss: 0.5554513995312462\n",
            "\n",
            "\n",
            "Training Epoch 198\n",
            "Micro f1: 0.753234089579406\n",
            "Macro f1: 0.7391954105291652\n",
            "accuracy: 0.7532340895794059\n",
            "Loss: 0.5040309070890262\n",
            "Validation Epoch 198\n",
            "Micro f1: 0.7127991675338188\n",
            "Macro f1: 0.7025958779463064\n",
            "accuracy: 0.7127991675338189\n",
            "Loss: 0.5566320001216959\n",
            "\n",
            "\n",
            "Training Epoch 199\n",
            "Micro f1: 0.7515439121107717\n",
            "Macro f1: 0.7375259970300538\n",
            "accuracy: 0.7515439121107717\n",
            "Loss: 0.5003755381814903\n",
            "Validation Epoch 199\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.7083299734795541\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.5661524342111319\n",
            "\n",
            "\n",
            "Training Epoch 200\n",
            "Micro f1: 0.7501137619450042\n",
            "Macro f1: 0.735745541004313\n",
            "accuracy: 0.7501137619450042\n",
            "Loss: 0.49982803098067424\n",
            "Validation Epoch 200\n",
            "Micro f1: 0.7263267429760667\n",
            "Macro f1: 0.708283702677747\n",
            "accuracy: 0.7263267429760666\n",
            "Loss: 0.5574675379705823\n",
            "\n",
            "\n",
            "Training Epoch 201\n",
            "Micro f1: 0.7559643762595074\n",
            "Macro f1: 0.7422775534665862\n",
            "accuracy: 0.7559643762595073\n",
            "Loss: 0.5031235635962159\n",
            "Validation Epoch 201\n",
            "Micro f1: 0.7195629552549427\n",
            "Macro f1: 0.6952378991155362\n",
            "accuracy: 0.7195629552549427\n",
            "Loss: 0.5564075331549999\n",
            "\n",
            "\n",
            "Training Epoch 202\n",
            "Micro f1: 0.7585646492881752\n",
            "Macro f1: 0.7448292293744256\n",
            "accuracy: 0.7585646492881752\n",
            "Loss: 0.49842562974118887\n",
            "Validation Epoch 202\n",
            "Micro f1: 0.722684703433923\n",
            "Macro f1: 0.7065256283655663\n",
            "accuracy: 0.722684703433923\n",
            "Loss: 0.5540615812806059\n",
            "\n",
            "\n",
            "Training Epoch 203\n",
            "Micro f1: 0.7479035298706365\n",
            "Macro f1: 0.7335615916839049\n",
            "accuracy: 0.7479035298706365\n",
            "Loss: 0.504171049334651\n",
            "Validation Epoch 203\n",
            "Micro f1: 0.7174817898022893\n",
            "Macro f1: 0.6937531746471906\n",
            "accuracy: 0.7174817898022893\n",
            "Loss: 0.5603098600856529\n",
            "\n",
            "\n",
            "Training Epoch 204\n",
            "Micro f1: 0.7519339530650718\n",
            "Macro f1: 0.7378714068999738\n",
            "accuracy: 0.7519339530650718\n",
            "Loss: 0.5037406922285126\n",
            "Validation Epoch 204\n",
            "Micro f1: 0.7200832466181063\n",
            "Macro f1: 0.695314369228412\n",
            "accuracy: 0.7200832466181062\n",
            "Loss: 0.5665195035047768\n",
            "\n",
            "\n",
            "Training Epoch 205\n",
            "Micro f1: 0.7507638302021712\n",
            "Macro f1: 0.7365004938983177\n",
            "accuracy: 0.7507638302021712\n",
            "Loss: 0.5058963063353065\n",
            "Validation Epoch 205\n",
            "Micro f1: 0.7154006243496358\n",
            "Macro f1: 0.7041378555373792\n",
            "accuracy: 0.7154006243496358\n",
            "Loss: 0.5618217289447784\n",
            "\n",
            "\n",
            "Training Epoch 206\n",
            "Micro f1: 0.7537541441851395\n",
            "Macro f1: 0.739926896677555\n",
            "accuracy: 0.7537541441851394\n",
            "Loss: 0.4994247858093087\n",
            "Validation Epoch 206\n",
            "Micro f1: 0.7195629552549427\n",
            "Macro f1: 0.7033749341084838\n",
            "accuracy: 0.7195629552549427\n",
            "Loss: 0.5593452928972639\n",
            "\n",
            "\n",
            "Training Epoch 207\n",
            "Micro f1: 0.7566144445166743\n",
            "Macro f1: 0.742636504024694\n",
            "accuracy: 0.7566144445166743\n",
            "Loss: 0.4962549954033195\n",
            "Validation Epoch 207\n",
            "Micro f1: 0.7133194588969823\n",
            "Macro f1: 0.6993002843100761\n",
            "accuracy: 0.7133194588969823\n",
            "Loss: 0.5573856645871785\n",
            "\n",
            "\n",
            "Training Epoch 208\n",
            "Micro f1: 0.7493336800364038\n",
            "Macro f1: 0.7352749723407449\n",
            "accuracy: 0.7493336800364038\n",
            "Loss: 0.5015500437259178\n",
            "Validation Epoch 208\n",
            "Micro f1: 0.7169614984391259\n",
            "Macro f1: 0.702193739496995\n",
            "accuracy: 0.7169614984391259\n",
            "Loss: 0.5546167288437363\n",
            "\n",
            "\n",
            "Training Epoch 209\n",
            "Micro f1: 0.7539491646622896\n",
            "Macro f1: 0.7407580290830831\n",
            "accuracy: 0.7539491646622896\n",
            "Loss: 0.49663715922659\n",
            "Validation Epoch 209\n",
            "Micro f1: 0.7195629552549427\n",
            "Macro f1: 0.7014835658023458\n",
            "accuracy: 0.7195629552549427\n",
            "Loss: 0.5547543313385042\n",
            "\n",
            "\n",
            "Training Epoch 210\n",
            "Micro f1: 0.7569394786452578\n",
            "Macro f1: 0.7435095258142711\n",
            "accuracy: 0.7569394786452578\n",
            "Loss: 0.49617546753427344\n",
            "Validation Epoch 210\n",
            "Micro f1: 0.7216441207075962\n",
            "Macro f1: 0.7052717566550823\n",
            "accuracy: 0.7216441207075962\n",
            "Loss: 0.5547052929716662\n",
            "\n",
            "\n",
            "Training Epoch 211\n",
            "Micro f1: 0.7518039394136384\n",
            "Macro f1: 0.7374992205287274\n",
            "accuracy: 0.7518039394136384\n",
            "Loss: 0.4969178956939128\n",
            "Validation Epoch 211\n",
            "Micro f1: 0.7112382934443289\n",
            "Macro f1: 0.6986384189330257\n",
            "accuracy: 0.7112382934443289\n",
            "Loss: 0.5544327885405091\n",
            "\n",
            "\n",
            "Training Epoch 212\n",
            "Micro f1: 0.7566794513423909\n",
            "Macro f1: 0.7431381989422678\n",
            "accuracy: 0.7566794513423909\n",
            "Loss: 0.49479360785528925\n",
            "Validation Epoch 212\n",
            "Micro f1: 0.7159209157127991\n",
            "Macro f1: 0.700657894736842\n",
            "accuracy: 0.7159209157127991\n",
            "Loss: 0.5521613618066489\n",
            "\n",
            "\n",
            "Training Epoch 213\n",
            "Micro f1: 0.7529090554508223\n",
            "Macro f1: 0.7389764459329358\n",
            "accuracy: 0.7529090554508223\n",
            "Loss: 0.49631809720502323\n",
            "Validation Epoch 213\n",
            "Micro f1: 0.7180020811654526\n",
            "Macro f1: 0.7019530963890721\n",
            "accuracy: 0.7180020811654526\n",
            "Loss: 0.5541682727327032\n",
            "\n",
            "\n",
            "Training Epoch 214\n",
            "Micro f1: 0.7553143080023402\n",
            "Macro f1: 0.7419319567948182\n",
            "accuracy: 0.7553143080023402\n",
            "Loss: 0.4971590436483867\n",
            "Validation Epoch 214\n",
            "Micro f1: 0.7127991675338188\n",
            "Macro f1: 0.6962987135668035\n",
            "accuracy: 0.7127991675338189\n",
            "Loss: 0.55966550604371\n",
            "\n",
            "\n",
            "Training Epoch 215\n",
            "Micro f1: 0.7554443216537736\n",
            "Macro f1: 0.7424999295188961\n",
            "accuracy: 0.7554443216537736\n",
            "Loss: 0.4942409699151521\n",
            "Validation Epoch 215\n",
            "Micro f1: 0.7164412070759626\n",
            "Macro f1: 0.7028534567054063\n",
            "accuracy: 0.7164412070759626\n",
            "Loss: 0.557087029676792\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCsL1sDQ44Hr"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BnTs8H8vwG9"
      },
      "source": [
        "tests = [\n",
        "         \"I love you\", \n",
        "         \"nigga i dont know you\", \n",
        "         \"muslims are the terrorist and they are the reason world is now full of shits\", \n",
        "         \"i love my country\"\n",
        "         ]\n",
        "inputs = tokenizer(tests, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "inputs.to(device)\n",
        "outputs = model(**inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OufxXO8-B44P"
      },
      "source": [
        "outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41jSXxIoCGNd"
      },
      "source": [
        "torch.argmax(torch.nn.Softmax(dim=1)(outputs[0]), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsqd3dLaCKwK"
      },
      "source": [
        "dataset[\"train\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyTM03nJ-ETN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}